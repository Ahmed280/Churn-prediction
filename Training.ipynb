{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training for Music Streaming Churn Prediction\n",
        "\n",
        "This notebook handles the comprehensive training of multiple models for churn prediction\n",
        "with automatic model saving capabilities.\n",
        "\n",
        "**Author:** Ahmed Alghaith  \n",
        "**Date:** August 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ahmad\\.conda\\envs\\EDA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All available libraries imported successfully!\n",
            "üéµ Ready to analyze music streaming churn data...\n",
            "‚úÖ All modules imported successfully!\n",
            "üéµ Ready for model training with automatic saving...\n",
            "üîç Dependency Status:\n",
            "   MLflow: ‚úÖ Available\n",
            "   Optuna: ‚úÖ Available\n",
            "   TensorFlow: ‚úÖ Available\n",
            "   XGBoost: ‚úÖ Available\n"
          ]
        }
      ],
      "source": [
        "# Import all required modules\n",
        "from utils import *\n",
        "from MusicStreamingEventProcessor import MusicStreamingEventProcessor\n",
        "from split import prepare_training_data, get_model_configurations\n",
        "from eval import train_and_evaluate_models, robust_hyperparameter_tuning, optuna_hyperparameter_tuning\n",
        "import joblib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "# Import LSTM wrapper if available\n",
        "try:\n",
        "    from lstm_wrapper import KerasLSTMWrapper\n",
        "    LSTM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è LSTM wrapper not available - skipping LSTM models\")\n",
        "    LSTM_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from deployment import deploy_model\n",
        "    DEPLOYMENT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è deployment not available - skippingdeployment\")\n",
        "    DEPLOYMENT_AVAILABLE = False\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")\n",
        "print(\"üéµ Ready for model training with automatic saving...\")\n",
        "\n",
        "# Check available dependencies\n",
        "check_dependencies()\n",
        "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_tracking_uri(\"mlruns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading and processing data...\n",
            "‚úÖ Data loaded successfully!\n",
            "üìä Loaded 543,694 events\n",
            "\n",
            "üë• Unique users: 449\n",
            "üìã Columns: ['ts', 'userId', 'sessionId', 'page', 'auth', 'method', 'status', 'level', 'itemInSession', 'location', 'userAgent', 'lastName', 'firstName', 'registration', 'gender', 'artist', 'song', 'length']\n"
          ]
        }
      ],
      "source": [
        "# Load and process data\n",
        "print(\"üì• Loading and processing data...\")\n",
        "\n",
        "# Load your data here - replace 'customer_churn.json' with your actual file path\n",
        "try:\n",
        "    events_df = pd.read_json('customer_churn.json', lines=True)\n",
        "    print(\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"üìä Loaded {len(events_df):,} events\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Data file not found. Please update the file path above.\")\n",
        "    print(\"üí° Expected file: 'customer_churn.json'\")\n",
        "    \n",
        "print(f\"\\nüë• Unique users: {events_df['userId'].nunique()}\")\n",
        "print(f\"üìã Columns: {list(events_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üè≠ Processing events to create user features...\n",
            "üßπ Cleaning event data...\n",
            "   ‚úÖ Cleaning results:\n",
            "      Removed 0 events with missing userId\n",
            "      üö´ Removed 4126 explicit churn events (data leakage prevention)\n",
            "      Final events: 539,568\n",
            "      Unique users: 449\n",
            "      Date range: 2018-10-01 00:03:35 to 2018-12-01 00:01:06\n",
            "üîß Engineering comprehensive user features...\n",
            "   Processing 449 users...\n",
            "   ‚úÖ Engineered 20 features for 449 users\n",
            "üéØ Identifying churned users using PROVEN activity-based method\n",
            "   Prediction horizon: 7 days\n",
            "   Inactivity threshold: 30 days\n",
            "   Cutoff date for prediction: 2018-11-24 00:01:06\n",
            "   üìä Churn analysis results:\n",
            "      Total users: 445\n",
            "      Churned users: 49 (11.01%)\n",
            "      Active users: 396 (88.99%)\n",
            "\n",
            "‚úÖ Feature engineering completed!\n",
            "üë• Processed 449 users\n",
            "üìä Created 21 features\n",
            "\n",
            "üîç Sample User Features:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>total_events</th>\n",
              "      <th>unique_sessions</th>\n",
              "      <th>total_songs_played</th>\n",
              "      <th>avg_session_length</th>\n",
              "      <th>days_active</th>\n",
              "      <th>thumbs_up</th>\n",
              "      <th>thumbs_down</th>\n",
              "      <th>home_visits</th>\n",
              "      <th>settings_visits</th>\n",
              "      <th>...</th>\n",
              "      <th>add_friend</th>\n",
              "      <th>add_playlist</th>\n",
              "      <th>engagement_ratio</th>\n",
              "      <th>avg_daily_events</th>\n",
              "      <th>paid_events_ratio</th>\n",
              "      <th>last_level_paid</th>\n",
              "      <th>weekend_activity_ratio</th>\n",
              "      <th>peak_hour</th>\n",
              "      <th>session_variety</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>175</td>\n",
              "      <td>2534</td>\n",
              "      <td>33</td>\n",
              "      <td>2049</td>\n",
              "      <td>76.787879</td>\n",
              "      <td>60</td>\n",
              "      <td>112</td>\n",
              "      <td>13</td>\n",
              "      <td>104</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>39</td>\n",
              "      <td>55</td>\n",
              "      <td>0.086425</td>\n",
              "      <td>42.233333</td>\n",
              "      <td>0.445541</td>\n",
              "      <td>1</td>\n",
              "      <td>0.162983</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>3959</td>\n",
              "      <td>39</td>\n",
              "      <td>3382</td>\n",
              "      <td>101.512821</td>\n",
              "      <td>61</td>\n",
              "      <td>143</td>\n",
              "      <td>37</td>\n",
              "      <td>135</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>71</td>\n",
              "      <td>94</td>\n",
              "      <td>0.087143</td>\n",
              "      <td>64.901639</td>\n",
              "      <td>0.984845</td>\n",
              "      <td>1</td>\n",
              "      <td>0.134630</td>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>163</td>\n",
              "      <td>5901</td>\n",
              "      <td>36</td>\n",
              "      <td>5049</td>\n",
              "      <td>163.916667</td>\n",
              "      <td>61</td>\n",
              "      <td>242</td>\n",
              "      <td>35</td>\n",
              "      <td>205</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>81</td>\n",
              "      <td>149</td>\n",
              "      <td>0.085918</td>\n",
              "      <td>96.737705</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232503</td>\n",
              "      <td>21</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>246</td>\n",
              "      <td>3530</td>\n",
              "      <td>23</td>\n",
              "      <td>2998</td>\n",
              "      <td>153.478261</td>\n",
              "      <td>35</td>\n",
              "      <td>153</td>\n",
              "      <td>34</td>\n",
              "      <td>110</td>\n",
              "      <td>24</td>\n",
              "      <td>...</td>\n",
              "      <td>39</td>\n",
              "      <td>80</td>\n",
              "      <td>0.086686</td>\n",
              "      <td>100.857143</td>\n",
              "      <td>0.889235</td>\n",
              "      <td>1</td>\n",
              "      <td>0.164589</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>179</td>\n",
              "      <td>2611</td>\n",
              "      <td>20</td>\n",
              "      <td>2218</td>\n",
              "      <td>130.550000</td>\n",
              "      <td>61</td>\n",
              "      <td>94</td>\n",
              "      <td>19</td>\n",
              "      <td>96</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>45</td>\n",
              "      <td>59</td>\n",
              "      <td>0.083110</td>\n",
              "      <td>42.803279</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.072769</td>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  userId  total_events  unique_sessions  total_songs_played  \\\n",
              "0    175          2534               33                2049   \n",
              "1    100          3959               39                3382   \n",
              "2    163          5901               36                5049   \n",
              "3    246          3530               23                2998   \n",
              "4    179          2611               20                2218   \n",
              "\n",
              "   avg_session_length  days_active  thumbs_up  thumbs_down  home_visits  \\\n",
              "0           76.787879           60        112           13          104   \n",
              "1          101.512821           61        143           37          135   \n",
              "2          163.916667           61        242           35          205   \n",
              "3          153.478261           35        153           34          110   \n",
              "4          130.550000           61         94           19           96   \n",
              "\n",
              "   settings_visits  ...  add_friend  add_playlist  engagement_ratio  \\\n",
              "0               11  ...          39            55          0.086425   \n",
              "1               17  ...          71            94          0.087143   \n",
              "2               30  ...          81           149          0.085918   \n",
              "3               24  ...          39            80          0.086686   \n",
              "4               17  ...          45            59          0.083110   \n",
              "\n",
              "   avg_daily_events  paid_events_ratio  last_level_paid  \\\n",
              "0         42.233333           0.445541                1   \n",
              "1         64.901639           0.984845                1   \n",
              "2         96.737705           1.000000                1   \n",
              "3        100.857143           0.889235                1   \n",
              "4         42.803279           1.000000                1   \n",
              "\n",
              "   weekend_activity_ratio  peak_hour  session_variety  churn  \n",
              "0                0.162983          0               15      0  \n",
              "1                0.134630         18               15      0  \n",
              "2                0.232503         21               13      0  \n",
              "3                0.164589          0               15      0  \n",
              "4                0.072769         15               12      0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Churn Distribution:\n",
            "   Active users (0): 400\n",
            "   Churned users (1): 49\n",
            "   Churn rate: 10.91%\n"
          ]
        }
      ],
      "source": [
        "# Process events to create user features\n",
        "print(\"üè≠ Processing events to create user features...\")\n",
        "\n",
        "processor = MusicStreamingEventProcessor(\n",
        "    prediction_horizon_days=7,\n",
        "    inactive_threshold_days=30\n",
        ")\n",
        "\n",
        "try:\n",
        "    # 1) Clean the raw event data\n",
        "    cleaned_events = processor.clean_events(events_df)\n",
        "\n",
        "    # 2) Engineer user features up to the prediction cutoff\n",
        "    user_features_df = processor.engineer_user_features()\n",
        "\n",
        "    # 3) Identify churn labels for each user\n",
        "    churn_labels = processor.identify_churn_users()\n",
        "    user_features_df['churn'] = user_features_df['userId'].map(churn_labels).fillna(0).astype(int)\n",
        "\n",
        "    \n",
        "    print(f\"\\n‚úÖ Feature engineering completed!\")\n",
        "    print(f\"üë• Processed {len(user_features_df)} users\")\n",
        "    print(f\"üìä Created {len(user_features_df.columns)} features\")\n",
        "    \n",
        "    # Display sample features\n",
        "    print(\"\\nüîç Sample User Features:\")\n",
        "    display(user_features_df.head())\n",
        "    \n",
        "    # Show churn distribution\n",
        "    if 'churn' in user_features_df.columns:\n",
        "        churn_dist = user_features_df['churn'].value_counts()\n",
        "        churn_rate = user_features_df['churn'].mean()\n",
        "        print(f\"\\nüìä Churn Distribution:\")\n",
        "        print(f\"   Active users (0): {churn_dist.get(0, 0)}\")\n",
        "        print(f\"   Churned users (1): {churn_dist.get(1, 0)}\")\n",
        "        print(f\"   Churn rate: {churn_rate:.2%}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Feature engineering failed: {e}\")\n",
        "    print(\"üí° Please check your data format and try again\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splitting and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Preparing training data...\n",
            "üîÑ Preparing training data with leakage prevention...\n",
            "üïê Performing LEAK-FREE temporal split...\n",
            "   üîí Ensuring strict chronological order: Train ‚Üí Val ‚Üí Test\n",
            "‚ö†Ô∏è WARNING: Using random split as temporal proxy\n",
            "   üí° In production, use user registration date or first activity date\n",
            "   üìä Train (earliest): 288 users\n",
            "   üìä Val (middle):     72 users\n",
            "   üìä Test (latest):    89 users\n",
            "   ‚úÖ No user overlap between splits\n",
            "\n",
            "üîç Checking feature names for leakage indicators...\n",
            "üîç Validating for data leakage...\n",
            "‚ö†Ô∏è POTENTIAL LEAKAGE: Suspicious features found:\n",
            "   - paid_events_ratio\n",
            "   - last_level_paid\n",
            "üí° Consider removing these features or verifying they're leak-free\n",
            "‚ö†Ô∏è HIGH CORRELATIONS: Found 2 near-perfect feature correlations\n",
            "   - unique_sessions ‚Üî home_visits: 0.9993\n",
            "   - total_songs_played ‚Üî thumbs_up: 0.9927\n",
            "üí° After training, watch for these leakage indicators:\n",
            "   - Training accuracy > 95%\n",
            "   - Perfect validation scores (1.000)\n",
            "   - No gap between train and validation performance\n",
            "   - Unrealistically high precision/recall\n",
            "üîç Validating Training Features for ML compatibility...\n",
            "   ‚úÖ Training Features validation complete: (288, 19) -> (288, 19)\n",
            "   üî¢ All 19 columns are now numeric\n",
            "üîç Validating Validation Features for ML compatibility...\n",
            "   ‚úÖ Validation Features validation complete: (72, 19) -> (72, 19)\n",
            "   üî¢ All 19 columns are now numeric\n",
            "üîç Validating Test Features for ML compatibility...\n",
            "   ‚úÖ Test Features validation complete: (89, 19) -> (89, 19)\n",
            "   üî¢ All 19 columns are now numeric\n",
            "\n",
            "‚úÖ Data splits prepared and validated:\n",
            "   Train: 288 samples, 19 features\n",
            "   Validation: 72 samples, 19 features\n",
            "   Test: 89 samples, 19 features\n",
            "‚öñÔ∏è Handling class imbalance with leakage prevention...\n",
            "üîç Validating Training Features for ML compatibility...\n",
            "   ‚úÖ Training Features validation complete: (288, 19) -> (288, 19)\n",
            "   üî¢ All 19 columns are now numeric\n",
            "üîç Validating for data leakage...\n",
            "‚ö†Ô∏è POTENTIAL LEAKAGE: Suspicious features found:\n",
            "   - paid_events_ratio\n",
            "   - last_level_paid\n",
            "üí° Consider removing these features or verifying they're leak-free\n",
            "‚ö†Ô∏è HIGH CORRELATIONS: Found 2 near-perfect feature correlations\n",
            "   - unique_sessions ‚Üî home_visits: 0.9993\n",
            "   - total_songs_played ‚Üî thumbs_up: 0.9927\n",
            "üí° After training, watch for these leakage indicators:\n",
            "   - Training accuracy > 95%\n",
            "   - Perfect validation scores (1.000)\n",
            "   - No gap between train and validation performance\n",
            "   - Unrealistically high precision/recall\n",
            "\n",
            "üîç Training set analysis:\n",
            "Classes present: [0 1]\n",
            "Total samples: 288\n",
            "Class distribution: {0: np.int64(253), 1: np.int64(35)}\n",
            "‚úÖ Computed class weights: {np.int64(0): np.float64(0.5691699604743083), np.int64(1): np.float64(4.114285714285714)}\n",
            "üîç Validating Balanced Training Features for ML compatibility...\n",
            "   ‚úÖ Balanced Training Features validation complete: (70, 19) -> (70, 19)\n",
            "   üî¢ All 19 columns are now numeric\n",
            "‚úÖ Created balanced dataset:\n",
            "   Original: 288 samples\n",
            "   Balanced: 70 samples\n",
            "   New distribution: {0: np.int64(35), 1: np.int64(35)}\n",
            "\n",
            "‚úÖ Data preparation completed!\n",
            "üìä Final dataset sizes:\n",
            "   Training: 288 samples\n",
            "   Validation: 72 samples\n",
            "   Test: 89 samples\n",
            "   Features: 19\n",
            "\n",
            "üîß Class imbalance handling:\n",
            "   Balanced sampling: ‚úÖ\n",
            "   Class weights: ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data with proper splits and class imbalance handling\n",
        "print(\"üîÑ Preparing training data...\")\n",
        "\n",
        "try:\n",
        "    data_results = prepare_training_data(user_features_df)\n",
        "    \n",
        "    # Extract prepared data\n",
        "    X_train = data_results['X_train']\n",
        "    y_train = data_results['y_train']\n",
        "    X_val = data_results['X_val']\n",
        "    y_val = data_results['y_val']\n",
        "    X_test = data_results['X_test']\n",
        "    y_test = data_results['y_test']\n",
        "    feature_columns = data_results['feature_columns']\n",
        "    \n",
        "    print(f\"\\n‚úÖ Data preparation completed!\")\n",
        "    print(f\"üìä Final dataset sizes:\")\n",
        "    print(f\"   Training: {len(X_train)} samples\")\n",
        "    print(f\"   Validation: {len(X_val)} samples\")\n",
        "    print(f\"   Test: {len(X_test)} samples\")\n",
        "    print(f\"   Features: {len(feature_columns)}\")\n",
        "    \n",
        "    # Check if balanced data is available\n",
        "    has_balanced = 'X_train_balanced' in data_results and data_results['X_train_balanced'] is not None\n",
        "    has_weights = 'class_weights' in data_results and data_results['class_weights'] is not None\n",
        "    \n",
        "    print(f\"\\nüîß Class imbalance handling:\")\n",
        "    print(f\"   Balanced sampling: {'‚úÖ' if has_balanced else '‚ùå'}\")\n",
        "    print(f\"   Class weights: {'‚úÖ' if has_weights else '‚ùå'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data preparation failed: {e}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Configuration and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Configuring models for training...\n",
            "‚úÖ Created 12 model configurations with leakage prevention\n",
            "‚úÖ Created 12 model configurations\n",
            "\n",
            "üìã Models to train:\n",
            "   1. Random Forest (Class Weighted)\n",
            "   2. Random Forest (Balanced Data)\n",
            "   3. Logistic Regression (Class Weighted)\n",
            "   4. Logistic Regression (Balanced Data)\n",
            "   5. Gradient Boosting (Class Weighted)\n",
            "   6. Gradient Boosting (Balanced Data)\n",
            "   7. Decision Tree (Class Weighted)\n",
            "   8. Decision Tree (Balanced Data)\n",
            "   9. XGBClassifier (Class Weighted)\n",
            "   10. XGBClassifier (Balanced Data)\n",
            "   11. Simple LSTM (Class Weighted)\n",
            "   12. Simple LSTM (Balanced Data)\n",
            "\n",
            "üîß Starting model training with MLflow logging...\n",
            "ü§ñ Training multiple machine learning models...\n",
            "\n",
            "üîß Training Random Forest (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.917\n",
            "   Val F1: 0.400\n",
            "\n",
            "üîß Training Random Forest (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.588\n",
            "\n",
            "üîß Training Logistic Regression (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 0.924\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.588\n",
            "\n",
            "üîß Training Logistic Regression (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.917\n",
            "   Val F1: 0.667\n",
            "\n",
            "üîß Training Gradient Boosting (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.931\n",
            "   Val F1: 0.615\n",
            "\n",
            "üîß Training Gradient Boosting (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.632\n",
            "\n",
            "üîß Training Decision Tree (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 0.983\n",
            "   Val Accuracy: 0.792\n",
            "   Val F1: 0.348\n",
            "   ‚ö†Ô∏è Potential overfitting detected (gap: 0.191)\n",
            "\n",
            "üîß Training Decision Tree (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.632\n",
            "\n",
            "üîß Training XGBClassifier (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 0.993\n",
            "   Val Accuracy: 0.931\n",
            "   Val F1: 0.706\n",
            "\n",
            "üîß Training XGBClassifier (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.917\n",
            "   Val F1: 0.667\n",
            "\n",
            "üîß Training Simple LSTM (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   üìä LSTM input shape: (288, 19)\n",
            "   üìä Target shape: (288,)\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 0.844\n",
            "   Val Accuracy: 0.875\n",
            "   Val F1: 0.571\n",
            "\n",
            "üîß Training Simple LSTM (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   üìä LSTM input shape: (70, 19)\n",
            "   üìä Target shape: (70,)\n",
            "   ‚úÖ Training successful!\n",
            "   Train Accuracy: 0.829\n",
            "   Val Accuracy: 0.847\n",
            "   Val F1: 0.421\n",
            "\n",
            "üìä Training Results Summary:\n",
            "Models attempted: 12\n",
            "Successful trainings: 12\n",
            "Failed trainings: 0\n",
            "\n",
            "\n",
            "üìä Training Results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Status</th>\n",
              "      <th>Train_Accuracy</th>\n",
              "      <th>Val_Accuracy</th>\n",
              "      <th>Val_Precision</th>\n",
              "      <th>Val_Recall</th>\n",
              "      <th>Val_F1</th>\n",
              "      <th>Training_Samples</th>\n",
              "      <th>Classes_in_Training</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logistic Regression (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.923611</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logistic Regression (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gradient Boosting (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Decision Tree (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.982639</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.347826</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Decision Tree (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>XGBClassifier (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.993056</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>XGBClassifier (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Simple LSTM (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Simple LSTM (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.828571</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Model   Status  Train_Accuracy  \\\n",
              "0         Random Forest (Class Weighted)  Success        1.000000   \n",
              "1          Random Forest (Balanced Data)  Success        1.000000   \n",
              "2   Logistic Regression (Class Weighted)  Success        0.923611   \n",
              "3    Logistic Regression (Balanced Data)  Success        1.000000   \n",
              "4     Gradient Boosting (Class Weighted)  Success        1.000000   \n",
              "5      Gradient Boosting (Balanced Data)  Success        1.000000   \n",
              "6         Decision Tree (Class Weighted)  Success        0.982639   \n",
              "7          Decision Tree (Balanced Data)  Success        1.000000   \n",
              "8         XGBClassifier (Class Weighted)  Success        0.993056   \n",
              "9          XGBClassifier (Balanced Data)  Success        1.000000   \n",
              "10          Simple LSTM (Class Weighted)  Success        0.843750   \n",
              "11           Simple LSTM (Balanced Data)  Success        0.828571   \n",
              "\n",
              "    Val_Accuracy  Val_Precision  Val_Recall    Val_F1  Training_Samples  \\\n",
              "0       0.916667       0.500000    0.333333  0.400000               288   \n",
              "1       0.902778       0.454545    0.833333  0.588235                70   \n",
              "2       0.902778       0.454545    0.833333  0.588235               288   \n",
              "3       0.916667       0.500000    1.000000  0.666667                70   \n",
              "4       0.930556       0.571429    0.666667  0.615385               288   \n",
              "5       0.902778       0.461538    1.000000  0.631579                70   \n",
              "6       0.791667       0.235294    0.666667  0.347826               288   \n",
              "7       0.902778       0.461538    1.000000  0.631579                70   \n",
              "8       0.930556       0.545455    1.000000  0.705882               288   \n",
              "9       0.916667       0.500000    1.000000  0.666667                70   \n",
              "10      0.875000       0.400000    1.000000  0.571429               288   \n",
              "11      0.847222       0.307692    0.666667  0.421053                70   \n",
              "\n",
              "    Classes_in_Training  \n",
              "0                     2  \n",
              "1                     2  \n",
              "2                     2  \n",
              "3                     2  \n",
              "4                     2  \n",
              "5                     2  \n",
              "6                     2  \n",
              "7                     2  \n",
              "8                     2  \n",
              "9                     2  \n",
              "10                    2  \n",
              "11                    2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/25 02:00:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèÜ Best performing model: XGBClassifier (Class Weighted)\n",
            "   Validation F1: 0.7059\n",
            "   Validation Accuracy: 0.9306\n",
            "üíæ Saving best model 'XGBClassifier (Class Weighted)' to MLflow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/25 02:01:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        }
      ],
      "source": [
        "# Configure and train multiple models\n",
        "print(\"ü§ñ Configuring models for training...\")\n",
        "\n",
        "try:\n",
        "    # Get model configurations\n",
        "    models_config = get_model_configurations(data_results)\n",
        "    print(f\"‚úÖ Created {len(models_config)} model configurations\")\n",
        "\n",
        "    # List configured models\n",
        "    print(\"\\nüìã Models to train:\")\n",
        "    for i, model_name in enumerate(models_config.keys(), 1):\n",
        "        print(f\"   {i}. {model_name}\")\n",
        "\n",
        "    trained_models = {}\n",
        "    print(\"\\nüîß Starting model training with MLflow logging...\")\n",
        "\n",
        "    mlflow.set_experiment(\"churn_model_training\")\n",
        "    with mlflow.start_run(run_name=\"train_all_models\"):\n",
        "        mlflow.log_param(\"num_models\", len(models_config))\n",
        "\n",
        "        # Train and evaluate all models\n",
        "        trained_models, results_df = train_and_evaluate_models(models_config, X_val, y_val)\n",
        "\n",
        "        # Log performance metrics for each\n",
        "        for _, row in results_df.iterrows():\n",
        "            if row.Status == \"Success\":\n",
        "                # sanitize model name\n",
        "                safe_name = row.Model.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "                mlflow.log_metric(f\"{safe_name}_val_f1\", float(row.Val_F1))\n",
        "                mlflow.log_metric(f\"{safe_name}_val_accuracy\", float(row.Val_Accuracy))\n",
        "\n",
        "        print(\"\\nüìä Training Results:\")\n",
        "        display(results_df)\n",
        "\n",
        "        # Identify and log the best model\n",
        "        successful = results_df[results_df.Status == \"Success\"]\n",
        "        if not successful.empty:\n",
        "            best_idx = successful.Val_F1.idxmax()\n",
        "            best_model_name = successful.loc[best_idx, \"Model\"]\n",
        "            best_f1 = float(successful.loc[best_idx, \"Val_F1\"])\n",
        "            best_acc = float(successful.loc[best_idx, \"Val_Accuracy\"])\n",
        "            mlflow.log_metric(\"best_model_val_f1\", best_f1)\n",
        "            mlflow.log_metric(\"best_model_val_accuracy\", best_acc)\n",
        "            mlflow.log_param(\"best_model_name\", best_model_name)\n",
        "\n",
        "            print(f\"\\nüèÜ Best performing model: {best_model_name}\")\n",
        "            print(f\"   Validation F1: {best_f1:.4f}\")\n",
        "            print(f\"   Validation Accuracy: {best_acc:.4f}\")\n",
        "            best_model = trained_models[best_model_name]\n",
        "        else:\n",
        "            print(\"‚ùå No models trained successfully!\")\n",
        "            best_model = None\n",
        "\n",
        "    print(f\"üíæ Saving best model '{best_model_name}' to MLflow\")\n",
        "    if best_model is not None:\n",
        "        mlflow.sklearn.log_model(best_model, \"best_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model training failed: {e}\")\n",
        "    print(\"üí° Check data preparation and model configurations\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-25 02:01:08,223] A new study created in memory with name: no-name-432992b8-5e75-4d2d-a198-a2099f82e7bf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Starting hyperparameter tuning on the best model only...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-25 02:01:08,553] Trial 0 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 235, 'learning_rate': 0.017351135471907687, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.9300320009190989, 'colsample_bytree': 0.9422352567087249, 'gamma': 4.151588748526681, 'reg_alpha': 1.9855125791462247, 'reg_lambda': 6.371234096524346}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:08,960] Trial 1 finished with value: 0.0 and parameters: {'n_estimators': 365, 'learning_rate': 0.0017391499475844616, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.8462058267859722, 'colsample_bytree': 0.5874533801279715, 'gamma': 4.349093182733871, 'reg_alpha': 4.006101385026467, 'reg_lambda': 4.00266275540874}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:09,418] Trial 2 finished with value: 0.0 and parameters: {'n_estimators': 458, 'learning_rate': 0.0010304211637789217, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.7616831495747722, 'colsample_bytree': 0.8237408408759435, 'gamma': 3.889907738550833, 'reg_alpha': 7.896433095148212, 'reg_lambda': 6.108378423838879}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:09,799] Trial 3 finished with value: 0.0 and parameters: {'n_estimators': 343, 'learning_rate': 0.011344659121714793, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.938010756571137, 'colsample_bytree': 0.6105144240458096, 'gamma': 4.6666881713558865, 'reg_alpha': 8.221508188669839, 'reg_lambda': 1.0049342380675874}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:10,074] Trial 4 finished with value: 0.36363636363636365 and parameters: {'n_estimators': 115, 'learning_rate': 0.27308043884023264, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.8487604245363249, 'colsample_bytree': 0.8328143088110204, 'gamma': 2.819030625722801, 'reg_alpha': 5.35142059701874, 'reg_lambda': 9.006268741919794}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:10,517] Trial 5 finished with value: 0.0 and parameters: {'n_estimators': 540, 'learning_rate': 0.2665601032106087, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7167151671437966, 'colsample_bytree': 0.5550713986058603, 'gamma': 2.284985780999664, 'reg_alpha': 7.0533484709646475, 'reg_lambda': 5.398914810340877}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:10,812] Trial 6 finished with value: 0.0 and parameters: {'n_estimators': 158, 'learning_rate': 0.001310844514121673, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.6358538692837121, 'colsample_bytree': 0.6108893855889483, 'gamma': 1.9028983966175173, 'reg_alpha': 7.633346790026523, 'reg_lambda': 8.95384131771923}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:11,127] Trial 7 finished with value: 0.0 and parameters: {'n_estimators': 146, 'learning_rate': 0.0018610469579320142, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.9603992431624038, 'colsample_bytree': 0.6584596268489046, 'gamma': 3.299872344625911, 'reg_alpha': 5.273472893247649, 'reg_lambda': 6.8936744475864264}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:11,500] Trial 8 finished with value: 0.0 and parameters: {'n_estimators': 354, 'learning_rate': 0.20602462149219353, 'max_depth': 10, 'min_child_weight': 4, 'subsample': 0.9081226631792216, 'colsample_bytree': 0.8152902604358006, 'gamma': 3.427210857097048, 'reg_alpha': 9.302570391023082, 'reg_lambda': 1.9752732290843622}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:11,936] Trial 9 finished with value: 0.0 and parameters: {'n_estimators': 456, 'learning_rate': 0.01719996606240023, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.6962375379763105, 'colsample_bytree': 0.5846381017095976, 'gamma': 3.1075228711108425, 'reg_alpha': 7.688624825890589, 'reg_lambda': 6.838135812669545}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:12,270] Trial 10 finished with value: 0.0 and parameters: {'n_estimators': 201, 'learning_rate': 0.04829977924590036, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.5331234951211176, 'colsample_bytree': 0.9975030623488442, 'gamma': 0.3786630821512311, 'reg_alpha': 0.25741501100588504, 'reg_lambda': 3.7039888244339827}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:12,557] Trial 11 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 85, 'learning_rate': 0.07210111392724072, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.8333390203038746, 'colsample_bytree': 0.9469132525112842, 'gamma': 1.4843922178031017, 'reg_alpha': 1.9379817117704197, 'reg_lambda': 9.540737838579632}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:12,916] Trial 12 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 263, 'learning_rate': 0.05862009047780353, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9994954852771448, 'colsample_bytree': 0.9948688904610059, 'gamma': 1.3476132002629468, 'reg_alpha': 1.3103372370153439, 'reg_lambda': 9.74961123082632}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:13,200] Trial 13 finished with value: 0.0 and parameters: {'n_estimators': 55, 'learning_rate': 0.04997766627396336, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.8271490004615648, 'colsample_bytree': 0.918753823530168, 'gamma': 1.0136267941772301, 'reg_alpha': 2.55738723126239, 'reg_lambda': 7.569522459763418}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:13,604] Trial 14 finished with value: 0.0 and parameters: {'n_estimators': 266, 'learning_rate': 0.0068934985867909965, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.7952166034613476, 'colsample_bytree': 0.9106016937781907, 'gamma': 1.7111687289366455, 'reg_alpha': 2.9610607079617015, 'reg_lambda': 8.142002436456458}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:13,889] Trial 15 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 83, 'learning_rate': 0.10431703495235023, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.8957092161772661, 'colsample_bytree': 0.7384305041117674, 'gamma': 0.34471654782462924, 'reg_alpha': 1.578554225798014, 'reg_lambda': 3.6387969223543717}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:14,257] Trial 16 finished with value: 0.0 and parameters: {'n_estimators': 214, 'learning_rate': 0.005721660190660987, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8840007402466934, 'colsample_bytree': 0.911014564912574, 'gamma': 2.3781305682590954, 'reg_alpha': 3.222299156409962, 'reg_lambda': 5.118057658159903}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:14,669] Trial 17 finished with value: 0.3333333333333333 and parameters: {'n_estimators': 258, 'learning_rate': 0.026714938699359245, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.9982833420715276, 'colsample_bytree': 0.7375040061906485, 'gamma': 4.059307608659683, 'reg_alpha': 0.11389497827488371, 'reg_lambda': 9.842111700635918}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:15,000] Trial 18 finished with value: 0.0 and parameters: {'n_estimators': 186, 'learning_rate': 0.1134211455686369, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.6333635366056785, 'colsample_bytree': 0.9480459048088355, 'gamma': 4.884403816932273, 'reg_alpha': 4.298027745868172, 'reg_lambda': 8.089992383603736}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:15,332] Trial 19 finished with value: 0.2222222222222222 and parameters: {'n_estimators': 111, 'learning_rate': 0.023230936163894107, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.7884038727597492, 'colsample_bytree': 0.8717337742594279, 'gamma': 0.9681063623564056, 'reg_alpha': 1.6706275399808075, 'reg_lambda': 2.746859532259829}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:15,871] Trial 20 finished with value: 0.0 and parameters: {'n_estimators': 599, 'learning_rate': 0.006056033590808104, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.5145383647096461, 'colsample_bytree': 0.7851034054349915, 'gamma': 3.6669876056493416, 'reg_alpha': 6.353940662115914, 'reg_lambda': 6.086801517880827}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:16,240] Trial 21 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 279, 'learning_rate': 0.045257445903274365, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9895683635560871, 'colsample_bytree': 0.9917763305003174, 'gamma': 1.5303423979951034, 'reg_alpha': 1.4588090231368318, 'reg_lambda': 9.648196012491486}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:16,641] Trial 22 finished with value: 0.4 and parameters: {'n_estimators': 294, 'learning_rate': 0.09092316748003397, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9232240450077355, 'colsample_bytree': 0.9626205115859302, 'gamma': 1.0687351304032953, 'reg_alpha': 0.9524969748349126, 'reg_lambda': 8.688585963942092}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:17,064] Trial 23 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 413, 'learning_rate': 0.05864268437946817, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.9620171678597467, 'colsample_bytree': 0.8805442208649192, 'gamma': 1.361506133423949, 'reg_alpha': 2.687136272236825, 'reg_lambda': 9.824406495278511}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:17,433] Trial 24 finished with value: 0.0 and parameters: {'n_estimators': 207, 'learning_rate': 0.02896709487225199, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.8638298424964498, 'colsample_bytree': 0.9577446695401394, 'gamma': 2.071470998598236, 'reg_alpha': 2.1974575495117845, 'reg_lambda': 7.403814837669244}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:17,809] Trial 25 finished with value: 0.0 and parameters: {'n_estimators': 239, 'learning_rate': 0.0128667023522622, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.9439851847847198, 'colsample_bytree': 0.8672814578493084, 'gamma': 2.645638422941123, 'reg_alpha': 4.172786554272959, 'reg_lambda': 8.724064190847685}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:18,188] Trial 26 finished with value: 0.5 and parameters: {'n_estimators': 303, 'learning_rate': 0.14328559184215825, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.8140166045394802, 'colsample_bytree': 0.5007055700713686, 'gamma': 0.05402931706223035, 'reg_alpha': 0.679104641346823, 'reg_lambda': 6.269409205066632}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:18,532] Trial 27 finished with value: 0.5 and parameters: {'n_estimators': 153, 'learning_rate': 0.07576521637174186, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.8826262906068945, 'colsample_bytree': 0.9363826145084565, 'gamma': 1.2521890840052896, 'reg_alpha': 3.514526391828456, 'reg_lambda': 7.940819060707199}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:18,810] Trial 28 finished with value: 0.6153846153846154 and parameters: {'n_estimators': 53, 'learning_rate': 0.03575444965576615, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.9827383849264882, 'colsample_bytree': 0.6832799991748864, 'gamma': 0.6286786878705095, 'reg_alpha': 2.0341793420816083, 'reg_lambda': 0.08915426807535898}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:19,244] Trial 29 finished with value: 0.0 and parameters: {'n_estimators': 393, 'learning_rate': 0.002721494079693761, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.8531255679939567, 'colsample_bytree': 0.9962594903446074, 'gamma': 1.8859916266840886, 'reg_alpha': 1.0732749591522328, 'reg_lambda': 4.568507000340742}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:19,546] Trial 30 finished with value: 0.0 and parameters: {'n_estimators': 112, 'learning_rate': 0.1757099380164126, 'max_depth': 7, 'min_child_weight': 9, 'subsample': 0.9236981019038621, 'colsample_bytree': 0.8907260568037948, 'gamma': 4.306712197963656, 'reg_alpha': 3.7575913500146445, 'reg_lambda': 9.277243922199762}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:19,852] Trial 31 finished with value: 0.5714285714285714 and parameters: {'n_estimators': 83, 'learning_rate': 0.07412523819768486, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.8915484241547045, 'colsample_bytree': 0.7321816744607719, 'gamma': 0.4951574626518612, 'reg_alpha': 1.5847271326657038, 'reg_lambda': 3.5737019070154488}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:20,176] Trial 32 finished with value: 0.5 and parameters: {'n_estimators': 165, 'learning_rate': 0.13002166305764884, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9626779339401987, 'colsample_bytree': 0.6824121644447111, 'gamma': 0.7175288400485571, 'reg_alpha': 2.2510697438022267, 'reg_lambda': 2.8816760347216}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:20,562] Trial 33 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 316, 'learning_rate': 0.08377295223515366, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.7706706492684025, 'colsample_bytree': 0.781163540112184, 'gamma': 0.05089639884342567, 'reg_alpha': 0.8302528934770608, 'reg_lambda': 4.2922936274276005}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:20,963] Trial 34 finished with value: 0.0 and parameters: {'n_estimators': 238, 'learning_rate': 0.00990995668645497, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.900830725698356, 'colsample_bytree': 0.83254652483748, 'gamma': 1.4743365807858213, 'reg_alpha': 4.685477269299728, 'reg_lambda': 2.034276786803429}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:21,267] Trial 35 finished with value: 0.0 and parameters: {'n_estimators': 81, 'learning_rate': 0.01662786455981531, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.7326396505499999, 'colsample_bytree': 0.9722487093657071, 'gamma': 2.8177240023209897, 'reg_alpha': 1.6189202836235452, 'reg_lambda': 5.883367282599855}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:21,598] Trial 36 finished with value: 0.4 and parameters: {'n_estimators': 128, 'learning_rate': 0.033680777375502964, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.8293431643550749, 'colsample_bytree': 0.7701443573386046, 'gamma': 0.3422367951741459, 'reg_alpha': 0.009938488531264555, 'reg_lambda': 4.904123607172427}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:21,896] Trial 37 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 88, 'learning_rate': 0.2998662235638511, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.9399285883154099, 'colsample_bytree': 0.8449074994972497, 'gamma': 1.994660538942973, 'reg_alpha': 3.2292083773489844, 'reg_lambda': 3.0393377461167033}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:22,302] Trial 38 finished with value: 0.5714285714285714 and parameters: {'n_estimators': 337, 'learning_rate': 0.21164533469408325, 'max_depth': 2, 'min_child_weight': 8, 'subsample': 0.8485691790175636, 'colsample_bytree': 0.7003030577020144, 'gamma': 0.8769873177437797, 'reg_alpha': 6.036187365810996, 'reg_lambda': 7.027635267152429}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:22,635] Trial 39 finished with value: 0.5714285714285714 and parameters: {'n_estimators': 175, 'learning_rate': 0.10694385134986802, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.8678084889052252, 'colsample_bytree': 0.6358035792780822, 'gamma': 2.2080886842602263, 'reg_alpha': 1.2155619730014133, 'reg_lambda': 9.207223427832705}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:23,046] Trial 40 finished with value: 0.0 and parameters: {'n_estimators': 370, 'learning_rate': 0.06301949051820017, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.9177813480551158, 'colsample_bytree': 0.9397763391415396, 'gamma': 4.491182900732947, 'reg_alpha': 9.37915550694225, 'reg_lambda': 8.353511069125751}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:23,442] Trial 41 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 279, 'learning_rate': 0.03978172452430812, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.977400319276345, 'colsample_bytree': 0.9820605887964723, 'gamma': 1.4966501959225331, 'reg_alpha': 1.7635789350148035, 'reg_lambda': 9.537620689670241}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:23,824] Trial 42 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 231, 'learning_rate': 0.02179279251491161, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9904958530001401, 'colsample_bytree': 0.9973293007243005, 'gamma': 1.6463989834597335, 'reg_alpha': 2.4978224802909876, 'reg_lambda': 9.834942524020923}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:24,196] Trial 43 finished with value: 0.4 and parameters: {'n_estimators': 256, 'learning_rate': 0.042495168190554135, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.9482522615639954, 'colsample_bytree': 0.9302321850101868, 'gamma': 1.1903217947781688, 'reg_alpha': 0.41516677742519403, 'reg_lambda': 8.756472855036689}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:24,666] Trial 44 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 501, 'learning_rate': 0.05833650960200369, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.9666849183008808, 'colsample_bytree': 0.9708040928412527, 'gamma': 1.767449995862707, 'reg_alpha': 1.399969032494149, 'reg_lambda': 9.121354050501266}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:25,008] Trial 45 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 128, 'learning_rate': 0.1698242423671825, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.999350032597777, 'colsample_bytree': 0.9116170996361422, 'gamma': 3.0815098363087343, 'reg_alpha': 0.6224356703980984, 'reg_lambda': 1.7140113918310664}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:25,369] Trial 46 finished with value: 0.0 and parameters: {'n_estimators': 279, 'learning_rate': 0.10103020728312138, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.9251482575243548, 'colsample_bytree': 0.809617875799104, 'gamma': 2.595800070785284, 'reg_alpha': 8.695199215513771, 'reg_lambda': 5.648169130480415}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:25,818] Trial 47 finished with value: 0.0 and parameters: {'n_estimators': 321, 'learning_rate': 0.003833571341470992, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.8981575674788062, 'colsample_bytree': 0.8989148108964653, 'gamma': 0.7853390840498582, 'reg_alpha': 2.8783680214419864, 'reg_lambda': 6.412183507735907}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:26,178] Trial 48 finished with value: 0.0 and parameters: {'n_estimators': 190, 'learning_rate': 0.00900665420150638, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.9437572929313188, 'colsample_bytree': 0.8535811771912508, 'gamma': 3.7421148233056476, 'reg_alpha': 1.9295308014850068, 'reg_lambda': 9.987994578484711}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:26,607] Trial 49 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 368, 'learning_rate': 0.04526573182420676, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.687291154964625, 'colsample_bytree': 0.9506141429083752, 'gamma': 0.23306414557495808, 'reg_alpha': 1.1711202004137728, 'reg_lambda': 7.541212690500425}. Best is trial 0 with value: 0.6666666666666666.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Tuned XGBClassifier (Class Weighted)\n",
            "   üìä Best F1: 0.6667\n",
            "   üîß Params: {'n_estimators': 235, 'learning_rate': 0.017351135471907687, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.9300320009190989, 'colsample_bytree': 0.9422352567087249, 'gamma': 4.151588748526681, 'reg_alpha': 1.9855125791462247, 'reg_lambda': 6.371234096524346}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/25 02:01:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/25 02:01:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Saving tuned model locally...\n",
            "   ‚úÖ Saved tuned model as: tuned_XGBClassifier_Class_Weighted_20250825_020132.joblib\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# End any active run\n",
        "if mlflow.active_run():\n",
        "    mlflow.end_run()\n",
        "\n",
        "print(\"üîß Starting hyperparameter tuning on the best model only...\")\n",
        "\n",
        "if 'X_train' in locals() and 'y_train' in locals():\n",
        "    try:\n",
        "        if 'best_model_name' not in locals():\n",
        "            raise ValueError(\"best_model_name not defined\")\n",
        "\n",
        "        # Extract config\n",
        "        config = models_config[best_model_name]\n",
        "        model_instance = config[\"model\"]\n",
        "        X_train_data, y_train_data = config[\"data\"]\n",
        "\n",
        "        # Derive type\n",
        "        model_class = model_instance.__class__\n",
        "        if isinstance(model_instance, RandomForestClassifier):\n",
        "            model_type = \"random_forest\"\n",
        "        elif isinstance(model_instance, GradientBoostingClassifier):\n",
        "            model_type = \"gradient_boosting\"\n",
        "        elif isinstance(model_instance, XGBClassifier):\n",
        "            model_type = \"xgboost\"\n",
        "        elif isinstance(model_instance, LogisticRegression):\n",
        "            model_type = \"logistic_regression\"\n",
        "        elif isinstance(model_instance, DecisionTreeClassifier):\n",
        "            model_type = \"decision_tree\"\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model class: {model_class}\")\n",
        "\n",
        "        # Tune\n",
        "        best_model, best_score, best_params = optuna_hyperparameter_tuning(\n",
        "            model_class, model_type,\n",
        "            X_train_data, y_train_data,\n",
        "            X_val, y_val,\n",
        "            n_trials=50\n",
        "        )\n",
        "\n",
        "        print(f\"   ‚úÖ Tuned {best_model_name}\")\n",
        "        print(f\"   üìä Best F1: {best_score:.4f}\")\n",
        "        print(f\"   üîß Params: {best_params}\")\n",
        "\n",
        "        final_model = best_model\n",
        "        mlflow.log_metric(\"best_val_f1\", best_score)\n",
        "        for p, v in best_params.items():\n",
        "            mlflow.log_param(p, v)\n",
        "        mlflow.sklearn.log_model(final_model, \"tuned_model\")\n",
        "\n",
        "        # Save locally\n",
        "        print(\"\\nüíæ Saving tuned model locally...\")\n",
        "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        safe = best_model_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "        fname = f\"tuned_{safe}_{ts}.joblib\"\n",
        "        joblib.dump(final_model, fname)\n",
        "        print(f\"   ‚úÖ Saved tuned model as: {fname}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Tuning failed: {e}\")\n",
        "        final_model = best_model if 'best_model' in locals() else None\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please run data processing and initial training first\")\n",
        "    final_model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Evaluation and Comprehensive Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Evaluating final model on test set...\n",
            "üéØ Final Optimized Model Evaluation Results:\n",
            "   Accuracy:  0.9101\n",
            "   Precision: 0.9002\n",
            "   Recall:    0.9101\n",
            "   F1-Score:  0.9042\n",
            "   ROC-AUC:   0.9390\n",
            "\n",
            "Classification Report for Final Optimized Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95        81\n",
            "           1       0.50      0.38      0.43         8\n",
            "\n",
            "    accuracy                           0.91        89\n",
            "   macro avg       0.72      0.67      0.69        89\n",
            "weighted avg       0.90      0.91      0.90        89\n",
            "\n",
            "\n",
            "‚úÖ Model evaluation completed!\n",
            "\n",
            "üíæ COMPREHENSIVE MODEL SAVING\n",
            "==================================================\n",
            "‚úÖ Final model saved: models\\final_churn_model_20250825_020132.joblib\n",
            "‚úÖ Pickle backup saved: models\\final_churn_model_20250825_020132.pkl\n",
            "‚úÖ Complete model package: output_files\\model_package_20250825_020132.joblib\n",
            "‚úÖ Feature columns saved: output_files\\feature_columns_20250825_020132.json\n",
            "‚úÖ Comprehensive report: output_files\\model_report_20250825_020132.json\n",
            "‚úÖ Loading script saved: output_files\\load_model_20250825_020132.py\n",
            "üíæ Preparing model for production deployment...\n",
            "‚úÖ Saved model+metadata to ./churn_predictor_v1_production.pkl\n",
            "‚úÖ Model deployed using deployment module!\n",
            "\n",
            "üì¶ MODEL SAVING SUMMARY\n",
            "========================================\n",
            "ü§ñ Model Type: XGBClassifier\n",
            "üìä Test F1-Score: 0.9042\n",
            "üìä Test Accuracy: 0.9101\n",
            "üìä Test Precision: 0.9002\n",
            "üìä Test Recall: 0.9101\n",
            "üî¢ Features: 19\n",
            "üìÖ Saved: 2025-08-25 02:01:33\n",
            "üìÇ Files created: 7 (model, backup, package, features, report, script, metadata)\n",
            "\n",
            "‚úÖ MODEL SUCCESSFULLY SAVED AND READY FOR PRODUCTION!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the final model on the test set and save it comprehensively\n",
        "if final_model is not None and 'X_test' in locals():\n",
        "    print(\"üéØ Evaluating final model on test set...\")\n",
        "    \n",
        "    try:\n",
        "        from eval import evaluate_churn_model\n",
        "        \n",
        "        final_metrics = evaluate_churn_model(\n",
        "            model=final_model,\n",
        "            X_test=X_test,\n",
        "            y_test=y_test,\n",
        "            model_name=\"Final Optimized Model\"\n",
        "        )\n",
        "        \n",
        "        print(\"\\n‚úÖ Model evaluation completed!\")\n",
        "        \n",
        "        # Prepare comprehensive model information\n",
        "        model_info = {\n",
        "            'model': final_model,\n",
        "            'metrics': final_metrics,\n",
        "            'feature_columns': feature_columns,\n",
        "            'training_samples': len(X_train),\n",
        "            'validation_samples': len(X_val),\n",
        "            'test_samples': len(X_test),\n",
        "            'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'churn_rate': user_features_df['churn'].mean() if 'user_features_df' in locals() else 0.0,\n",
        "            'model_type': type(final_model).__name__,\n",
        "            'data_shape': X_train.shape\n",
        "        }\n",
        "        \n",
        "        # Comprehensive Model Saving\n",
        "        print(\"\\nüíæ COMPREHENSIVE MODEL SAVING\")\n",
        "        print(\"=\" * 50)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        \n",
        "        # 1. Save the final model (primary method)\n",
        "        model_filename = f\"models\\\\final_churn_model_{timestamp}.joblib\"\n",
        "        try:\n",
        "            joblib.dump(final_model, model_filename)\n",
        "            print(f\"‚úÖ Final model saved: {model_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Joblib save failed: {e}\")\n",
        "        \n",
        "        # 2. Save using pickle (backup method)\n",
        "        pickle_filename = f\"models\\\\final_churn_model_{timestamp}.pkl\"\n",
        "        try:\n",
        "            with open(pickle_filename, 'wb') as f:\n",
        "                pickle.dump(final_model, f)\n",
        "            print(f\"‚úÖ Pickle backup saved: {pickle_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Pickle save failed: {e}\")\n",
        "        \n",
        "        # 3. Save complete model information package\n",
        "        info_filename = f\"output_files\\\\model_package_{timestamp}.joblib\"\n",
        "        try:\n",
        "            joblib.dump(model_info, info_filename)\n",
        "            print(f\"‚úÖ Complete model package: {info_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Model package save failed: {e}\")\n",
        "        \n",
        "        # 4. Save feature columns (critical for deployment)\n",
        "        feature_filename = f\"output_files\\\\feature_columns_{timestamp}.json\"\n",
        "        try:\n",
        "            with open(feature_filename, 'w') as f:\n",
        "                json.dump(feature_columns, f, indent=2)\n",
        "            print(f\"‚úÖ Feature columns saved: {feature_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Feature columns save failed: {e}\")\n",
        "        \n",
        "        # 5. Save comprehensive metrics and metadata\n",
        "        metrics_filename = f\"output_files\\\\model_report_{timestamp}.json\"\n",
        "        try:\n",
        "            comprehensive_report = {\n",
        "                'model_info': {\n",
        "                    'type': type(final_model).__name__,\n",
        "                    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'version': '1.0'\n",
        "                },\n",
        "                'data_info': {\n",
        "                    'total_users': len(user_features_df) if 'user_features_df' in locals() else 0,\n",
        "                    'training_samples': len(X_train),\n",
        "                    'validation_samples': len(X_val),\n",
        "                    'test_samples': len(X_test),\n",
        "                    'features_count': len(feature_columns),\n",
        "                    'churn_rate': user_features_df['churn'].mean() if 'user_features_df' in locals() else 0.0\n",
        "                },\n",
        "                'performance': final_metrics,\n",
        "                'features': feature_columns\n",
        "            }\n",
        "            with open(metrics_filename, 'w') as f:\n",
        "                json.dump(comprehensive_report, f, indent=2)\n",
        "            print(f\"‚úÖ Comprehensive report: {metrics_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Report save failed: {e}\")\n",
        "        \n",
        "        # 6. Save model loading script\n",
        "        script_filename = f\"output_files\\\\load_model_{timestamp}.py\"\n",
        "        try:\n",
        "            loading_script = f'''#!/usr/bin/env python3\n",
        "\"\"\"\\nModel Loading Script for Churn Prediction Model\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\"\"\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_churn_model():\n",
        "    \"\"\"Load the trained churn prediction model and its metadata.\"\"\"\n",
        "    \n",
        "    # Load the model\n",
        "    model = joblib.load('{model_filename}')\n",
        "    \n",
        "    # Load feature columns\n",
        "    with open('{feature_filename}', 'r') as f:\n",
        "        feature_columns = json.load(f)\n",
        "    \n",
        "    # Load model report\n",
        "    with open('{metrics_filename}', 'r') as f:\n",
        "        model_report = json.load(f)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded model: {{model_report['model_info']['type']}}\")\n",
        "    print(f\"üìä Test F1 Score: {{model_report['performance']['f1']:.4f}}\")\n",
        "    print(f\"üìä Test Accuracy: {{model_report['performance']['accuracy']:.4f}}\")\n",
        "    print(f\"üî¢ Features: {{len(feature_columns)}}\")\n",
        "    \n",
        "    return model, feature_columns, model_report\n",
        "\n",
        "def predict_churn(model, feature_columns, user_data):\n",
        "    \"\"\"Make churn predictions on new user data.\"\"\"\n",
        "    \n",
        "    # Ensure user_data has the required columns\n",
        "    if isinstance(user_data, dict):\n",
        "        user_data = pd.DataFrame([user_data])\n",
        "    \n",
        "    # Reorder columns to match training data\n",
        "    user_data = user_data.reindex(columns=feature_columns, fill_value=0)\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(user_data)\n",
        "    probability = model.predict_proba(user_data)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    return prediction, probability\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    model, features, report = load_churn_model()\n",
        "    print(f\"\\\\nüéØ Model ready for predictions!\")\n",
        "    print(f\"Example: prediction, probability = predict_churn(model, features, user_data_dict)\")\n",
        "'''\n",
        "            with open(script_filename, 'w') as f:\n",
        "                f.write(loading_script)\n",
        "            print(f\"‚úÖ Loading script saved: {script_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Loading script save failed: {e}\")\n",
        "        \n",
        "        # 7. Try deployment module if available\n",
        "        if DEPLOYMENT_AVAILABLE:\n",
        "            try:\n",
        "                deploy_model(\n",
        "                    model=final_model,\n",
        "                    model_name=\"churn_predictor_v1\",\n",
        "                    feature_columns=feature_columns,\n",
        "                    performance_metrics=final_metrics\n",
        "                )\n",
        "                print(\"‚úÖ Model deployed using deployment module!\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Deployment module failed: {e}\")\n",
        "        \n",
        "        # Summary\n",
        "        print(\"\\nüì¶ MODEL SAVING SUMMARY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"ü§ñ Model Type: {type(final_model).__name__}\")\n",
        "        print(f\"üìä Test F1-Score: {final_metrics['f1']:.4f}\")\n",
        "        print(f\"üìä Test Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "        print(f\"üìä Test Precision: {final_metrics['precision']:.4f}\")\n",
        "        print(f\"üìä Test Recall: {final_metrics['recall']:.4f}\")\n",
        "        print(f\"üî¢ Features: {len(feature_columns)}\")\n",
        "        print(f\"üìÖ Saved: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"üìÇ Files created: 7 (model, backup, package, features, report, script, metadata)\")\n",
        "        \n",
        "        print(\"\\n‚úÖ MODEL SUCCESSFULLY SAVED AND READY FOR PRODUCTION!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Final evaluation/saving failed: {e}\")\n",
        "        \n",
        "        # Emergency save - save whatever we have\n",
        "        print(\"\\nüö® EMERGENCY SAVE PROCEDURE\")\n",
        "        if 'final_model' in locals() and final_model is not None:\n",
        "            emergency_filename = f\"models\\\\EMERGENCY_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "            try:\n",
        "                with open(emergency_filename, 'wb') as f:\n",
        "                    pickle.dump(final_model, f)\n",
        "                print(f\"üö® Emergency model saved: {emergency_filename}\")\n",
        "            except:\n",
        "                print(\"‚ùå Emergency save also failed\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No final model available or test data missing\")\n",
        "    print(\"üí° Please run all previous cells to train and tune a model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EDA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
