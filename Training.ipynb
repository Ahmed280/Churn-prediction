{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training for Music Streaming Churn Prediction\n",
        "\n",
        "This notebook handles the comprehensive training of multiple models for churn prediction\n",
        "with automatic model saving capabilities.\n",
        "\n",
        "**Author:** Ahmed Alghaith  \n",
        "**Date:** August 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ahmad\\.conda\\envs\\EDA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All available libraries imported successfully!\n",
            "🎵 Ready to analyze music streaming churn data...\n",
            "✅ All modules imported successfully!\n",
            "🎵 Ready for model training with automatic saving...\n",
            "🔍 Dependency Status:\n",
            "   MLflow: ✅ Available\n",
            "   Optuna: ✅ Available\n",
            "   TensorFlow: ✅ Available\n",
            "   XGBoost: ✅ Available\n"
          ]
        }
      ],
      "source": [
        "# Import all required modules\n",
        "from utils import *\n",
        "from MusicStreamingEventProcessor import MusicStreamingEventProcessor\n",
        "from split import prepare_training_data, get_model_configurations\n",
        "from eval import train_and_evaluate_models, robust_hyperparameter_tuning, optuna_hyperparameter_tuning\n",
        "import joblib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "# Import LSTM wrapper if available\n",
        "try:\n",
        "    from lstm_wrapper import KerasLSTMWrapper\n",
        "    LSTM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ LSTM wrapper not available - skipping LSTM models\")\n",
        "    LSTM_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from deployment import deploy_model\n",
        "    DEPLOYMENT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ deployment not available - skippingdeployment\")\n",
        "    DEPLOYMENT_AVAILABLE = False\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✅ All modules imported successfully!\")\n",
        "print(\"🎵 Ready for model training with automatic saving...\")\n",
        "\n",
        "# Check available dependencies\n",
        "check_dependencies()\n",
        "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_tracking_uri(\"mlruns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 Loading and processing data...\n",
            "✅ Data loaded successfully!\n",
            "📊 Loaded 543,694 events\n",
            "\n",
            "👥 Unique users: 449\n",
            "📋 Columns: ['ts', 'userId', 'sessionId', 'page', 'auth', 'method', 'status', 'level', 'itemInSession', 'location', 'userAgent', 'lastName', 'firstName', 'registration', 'gender', 'artist', 'song', 'length']\n"
          ]
        }
      ],
      "source": [
        "# Load and process data\n",
        "print(\"📥 Loading and processing data...\")\n",
        "\n",
        "# Load your data here - replace 'customer_churn.json' with your actual file path\n",
        "try:\n",
        "    events_df = pd.read_json('customer_churn.json', lines=True)\n",
        "    print(\"✅ Data loaded successfully!\")\n",
        "    print(f\"📊 Loaded {len(events_df):,} events\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Data file not found. Please update the file path above.\")\n",
        "    print(\"💡 Expected file: 'customer_churn.json'\")\n",
        "    \n",
        "print(f\"\\n👥 Unique users: {events_df['userId'].nunique()}\")\n",
        "print(f\"📋 Columns: {list(events_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏭 Processing events to create user features...\n",
            "🧹 Cleaning event data...\n",
            "   ✅ Cleaning results:\n",
            "      Removed 0 events with missing userId\n",
            "      🚫 Removed 4126 explicit churn events (data leakage prevention)\n",
            "      Final events: 539,568\n",
            "      Unique users: 449\n",
            "      Date range: 2018-10-01 00:03:35 to 2018-12-01 00:01:06\n",
            "🔧 Engineering comprehensive user features...\n",
            "   Processing 449 users...\n",
            "   ✅ Engineered 20 features for 449 users\n",
            "🎯 Identifying churned users using PROVEN activity-based method\n",
            "   Prediction horizon: 7 days\n",
            "   Inactivity threshold: 30 days\n",
            "   Cutoff date for prediction: 2018-11-24 00:01:06\n",
            "   📊 Churn analysis results:\n",
            "      Total users: 445\n",
            "      Churned users: 49 (11.01%)\n",
            "      Active users: 396 (88.99%)\n",
            "\n",
            "✅ Feature engineering completed!\n",
            "👥 Processed 449 users\n",
            "📊 Created 21 features\n",
            "\n",
            "🔍 Sample User Features:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>total_events</th>\n",
              "      <th>unique_sessions</th>\n",
              "      <th>total_songs_played</th>\n",
              "      <th>avg_session_length</th>\n",
              "      <th>days_active</th>\n",
              "      <th>thumbs_up</th>\n",
              "      <th>thumbs_down</th>\n",
              "      <th>home_visits</th>\n",
              "      <th>settings_visits</th>\n",
              "      <th>...</th>\n",
              "      <th>add_friend</th>\n",
              "      <th>add_playlist</th>\n",
              "      <th>engagement_ratio</th>\n",
              "      <th>avg_daily_events</th>\n",
              "      <th>paid_events_ratio</th>\n",
              "      <th>last_level_paid</th>\n",
              "      <th>weekend_activity_ratio</th>\n",
              "      <th>peak_hour</th>\n",
              "      <th>session_variety</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>175</td>\n",
              "      <td>2534</td>\n",
              "      <td>33</td>\n",
              "      <td>2049</td>\n",
              "      <td>76.787879</td>\n",
              "      <td>60</td>\n",
              "      <td>112</td>\n",
              "      <td>13</td>\n",
              "      <td>104</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>39</td>\n",
              "      <td>55</td>\n",
              "      <td>0.086425</td>\n",
              "      <td>42.233333</td>\n",
              "      <td>0.445541</td>\n",
              "      <td>1</td>\n",
              "      <td>0.162983</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>3959</td>\n",
              "      <td>39</td>\n",
              "      <td>3382</td>\n",
              "      <td>101.512821</td>\n",
              "      <td>61</td>\n",
              "      <td>143</td>\n",
              "      <td>37</td>\n",
              "      <td>135</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>71</td>\n",
              "      <td>94</td>\n",
              "      <td>0.087143</td>\n",
              "      <td>64.901639</td>\n",
              "      <td>0.984845</td>\n",
              "      <td>1</td>\n",
              "      <td>0.134630</td>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>163</td>\n",
              "      <td>5901</td>\n",
              "      <td>36</td>\n",
              "      <td>5049</td>\n",
              "      <td>163.916667</td>\n",
              "      <td>61</td>\n",
              "      <td>242</td>\n",
              "      <td>35</td>\n",
              "      <td>205</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>81</td>\n",
              "      <td>149</td>\n",
              "      <td>0.085918</td>\n",
              "      <td>96.737705</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.232503</td>\n",
              "      <td>21</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>246</td>\n",
              "      <td>3530</td>\n",
              "      <td>23</td>\n",
              "      <td>2998</td>\n",
              "      <td>153.478261</td>\n",
              "      <td>35</td>\n",
              "      <td>153</td>\n",
              "      <td>34</td>\n",
              "      <td>110</td>\n",
              "      <td>24</td>\n",
              "      <td>...</td>\n",
              "      <td>39</td>\n",
              "      <td>80</td>\n",
              "      <td>0.086686</td>\n",
              "      <td>100.857143</td>\n",
              "      <td>0.889235</td>\n",
              "      <td>1</td>\n",
              "      <td>0.164589</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>179</td>\n",
              "      <td>2611</td>\n",
              "      <td>20</td>\n",
              "      <td>2218</td>\n",
              "      <td>130.550000</td>\n",
              "      <td>61</td>\n",
              "      <td>94</td>\n",
              "      <td>19</td>\n",
              "      <td>96</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>45</td>\n",
              "      <td>59</td>\n",
              "      <td>0.083110</td>\n",
              "      <td>42.803279</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.072769</td>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  userId  total_events  unique_sessions  total_songs_played  \\\n",
              "0    175          2534               33                2049   \n",
              "1    100          3959               39                3382   \n",
              "2    163          5901               36                5049   \n",
              "3    246          3530               23                2998   \n",
              "4    179          2611               20                2218   \n",
              "\n",
              "   avg_session_length  days_active  thumbs_up  thumbs_down  home_visits  \\\n",
              "0           76.787879           60        112           13          104   \n",
              "1          101.512821           61        143           37          135   \n",
              "2          163.916667           61        242           35          205   \n",
              "3          153.478261           35        153           34          110   \n",
              "4          130.550000           61         94           19           96   \n",
              "\n",
              "   settings_visits  ...  add_friend  add_playlist  engagement_ratio  \\\n",
              "0               11  ...          39            55          0.086425   \n",
              "1               17  ...          71            94          0.087143   \n",
              "2               30  ...          81           149          0.085918   \n",
              "3               24  ...          39            80          0.086686   \n",
              "4               17  ...          45            59          0.083110   \n",
              "\n",
              "   avg_daily_events  paid_events_ratio  last_level_paid  \\\n",
              "0         42.233333           0.445541                1   \n",
              "1         64.901639           0.984845                1   \n",
              "2         96.737705           1.000000                1   \n",
              "3        100.857143           0.889235                1   \n",
              "4         42.803279           1.000000                1   \n",
              "\n",
              "   weekend_activity_ratio  peak_hour  session_variety  churn  \n",
              "0                0.162983          0               15      0  \n",
              "1                0.134630         18               15      0  \n",
              "2                0.232503         21               13      0  \n",
              "3                0.164589          0               15      0  \n",
              "4                0.072769         15               12      0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Churn Distribution:\n",
            "   Active users (0): 400\n",
            "   Churned users (1): 49\n",
            "   Churn rate: 10.91%\n"
          ]
        }
      ],
      "source": [
        "# Process events to create user features\n",
        "print(\"🏭 Processing events to create user features...\")\n",
        "\n",
        "processor = MusicStreamingEventProcessor(\n",
        "    prediction_horizon_days=7,\n",
        "    inactive_threshold_days=30\n",
        ")\n",
        "\n",
        "try:\n",
        "    # 1) Clean the raw event data\n",
        "    cleaned_events = processor.clean_events(events_df)\n",
        "\n",
        "    # 2) Engineer user features up to the prediction cutoff\n",
        "    user_features_df = processor.engineer_user_features()\n",
        "\n",
        "    # 3) Identify churn labels for each user\n",
        "    churn_labels = processor.identify_churn_users()\n",
        "    user_features_df['churn'] = user_features_df['userId'].map(churn_labels).fillna(0).astype(int)\n",
        "\n",
        "    \n",
        "    print(f\"\\n✅ Feature engineering completed!\")\n",
        "    print(f\"👥 Processed {len(user_features_df)} users\")\n",
        "    print(f\"📊 Created {len(user_features_df.columns)} features\")\n",
        "    \n",
        "    # Display sample features\n",
        "    print(\"\\n🔍 Sample User Features:\")\n",
        "    display(user_features_df.head())\n",
        "    \n",
        "    # Show churn distribution\n",
        "    if 'churn' in user_features_df.columns:\n",
        "        churn_dist = user_features_df['churn'].value_counts()\n",
        "        churn_rate = user_features_df['churn'].mean()\n",
        "        print(f\"\\n📊 Churn Distribution:\")\n",
        "        print(f\"   Active users (0): {churn_dist.get(0, 0)}\")\n",
        "        print(f\"   Churned users (1): {churn_dist.get(1, 0)}\")\n",
        "        print(f\"   Churn rate: {churn_rate:.2%}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Feature engineering failed: {e}\")\n",
        "    print(\"💡 Please check your data format and try again\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splitting and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Preparing training data...\n",
            "🔄 Preparing training data with leakage prevention...\n",
            "🕐 Performing LEAK-FREE temporal split...\n",
            "   🔒 Ensuring strict chronological order: Train → Val → Test\n",
            "⚠️ WARNING: Using random split as temporal proxy\n",
            "   💡 In production, use user registration date or first activity date\n",
            "   📊 Train (earliest): 288 users\n",
            "   📊 Val (middle):     72 users\n",
            "   📊 Test (latest):    89 users\n",
            "   ✅ No user overlap between splits\n",
            "\n",
            "🔍 Checking feature names for leakage indicators...\n",
            "🔍 Validating for data leakage...\n",
            "⚠️ POTENTIAL LEAKAGE: Suspicious features found:\n",
            "   - paid_events_ratio\n",
            "   - last_level_paid\n",
            "💡 Consider removing these features or verifying they're leak-free\n",
            "⚠️ HIGH CORRELATIONS: Found 2 near-perfect feature correlations\n",
            "   - unique_sessions ↔ home_visits: 0.9993\n",
            "   - total_songs_played ↔ thumbs_up: 0.9927\n",
            "💡 After training, watch for these leakage indicators:\n",
            "   - Training accuracy > 95%\n",
            "   - Perfect validation scores (1.000)\n",
            "   - No gap between train and validation performance\n",
            "   - Unrealistically high precision/recall\n",
            "🔍 Validating Training Features for ML compatibility...\n",
            "   ✅ Training Features validation complete: (288, 19) -> (288, 19)\n",
            "   🔢 All 19 columns are now numeric\n",
            "🔍 Validating Validation Features for ML compatibility...\n",
            "   ✅ Validation Features validation complete: (72, 19) -> (72, 19)\n",
            "   🔢 All 19 columns are now numeric\n",
            "🔍 Validating Test Features for ML compatibility...\n",
            "   ✅ Test Features validation complete: (89, 19) -> (89, 19)\n",
            "   🔢 All 19 columns are now numeric\n",
            "\n",
            "✅ Data splits prepared and validated:\n",
            "   Train: 288 samples, 19 features\n",
            "   Validation: 72 samples, 19 features\n",
            "   Test: 89 samples, 19 features\n",
            "⚖️ Handling class imbalance with leakage prevention...\n",
            "🔍 Validating Training Features for ML compatibility...\n",
            "   ✅ Training Features validation complete: (288, 19) -> (288, 19)\n",
            "   🔢 All 19 columns are now numeric\n",
            "🔍 Validating for data leakage...\n",
            "⚠️ POTENTIAL LEAKAGE: Suspicious features found:\n",
            "   - paid_events_ratio\n",
            "   - last_level_paid\n",
            "💡 Consider removing these features or verifying they're leak-free\n",
            "⚠️ HIGH CORRELATIONS: Found 2 near-perfect feature correlations\n",
            "   - unique_sessions ↔ home_visits: 0.9993\n",
            "   - total_songs_played ↔ thumbs_up: 0.9927\n",
            "💡 After training, watch for these leakage indicators:\n",
            "   - Training accuracy > 95%\n",
            "   - Perfect validation scores (1.000)\n",
            "   - No gap between train and validation performance\n",
            "   - Unrealistically high precision/recall\n",
            "\n",
            "🔍 Training set analysis:\n",
            "Classes present: [0 1]\n",
            "Total samples: 288\n",
            "Class distribution: {0: np.int64(253), 1: np.int64(35)}\n",
            "✅ Computed class weights: {np.int64(0): np.float64(0.5691699604743083), np.int64(1): np.float64(4.114285714285714)}\n",
            "🔍 Validating Balanced Training Features for ML compatibility...\n",
            "   ✅ Balanced Training Features validation complete: (70, 19) -> (70, 19)\n",
            "   🔢 All 19 columns are now numeric\n",
            "✅ Created balanced dataset:\n",
            "   Original: 288 samples\n",
            "   Balanced: 70 samples\n",
            "   New distribution: {0: np.int64(35), 1: np.int64(35)}\n",
            "\n",
            "✅ Data preparation completed!\n",
            "📊 Final dataset sizes:\n",
            "   Training: 288 samples\n",
            "   Validation: 72 samples\n",
            "   Test: 89 samples\n",
            "   Features: 19\n",
            "\n",
            "🔧 Class imbalance handling:\n",
            "   Balanced sampling: ✅\n",
            "   Class weights: ✅\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data with proper splits and class imbalance handling\n",
        "print(\"🔄 Preparing training data...\")\n",
        "\n",
        "try:\n",
        "    data_results = prepare_training_data(user_features_df)\n",
        "    \n",
        "    # Extract prepared data\n",
        "    X_train = data_results['X_train']\n",
        "    y_train = data_results['y_train']\n",
        "    X_val = data_results['X_val']\n",
        "    y_val = data_results['y_val']\n",
        "    X_test = data_results['X_test']\n",
        "    y_test = data_results['y_test']\n",
        "    feature_columns = data_results['feature_columns']\n",
        "    \n",
        "    print(f\"\\n✅ Data preparation completed!\")\n",
        "    print(f\"📊 Final dataset sizes:\")\n",
        "    print(f\"   Training: {len(X_train)} samples\")\n",
        "    print(f\"   Validation: {len(X_val)} samples\")\n",
        "    print(f\"   Test: {len(X_test)} samples\")\n",
        "    print(f\"   Features: {len(feature_columns)}\")\n",
        "    \n",
        "    # Check if balanced data is available\n",
        "    has_balanced = 'X_train_balanced' in data_results and data_results['X_train_balanced'] is not None\n",
        "    has_weights = 'class_weights' in data_results and data_results['class_weights'] is not None\n",
        "    \n",
        "    print(f\"\\n🔧 Class imbalance handling:\")\n",
        "    print(f\"   Balanced sampling: {'✅' if has_balanced else '❌'}\")\n",
        "    print(f\"   Class weights: {'✅' if has_weights else '❌'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Data preparation failed: {e}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Configuration and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Configuring models for training...\n",
            "✅ Created 12 model configurations with leakage prevention\n",
            "✅ Created 12 model configurations\n",
            "\n",
            "📋 Models to train:\n",
            "   1. Random Forest (Class Weighted)\n",
            "   2. Random Forest (Balanced Data)\n",
            "   3. Logistic Regression (Class Weighted)\n",
            "   4. Logistic Regression (Balanced Data)\n",
            "   5. Gradient Boosting (Class Weighted)\n",
            "   6. Gradient Boosting (Balanced Data)\n",
            "   7. Decision Tree (Class Weighted)\n",
            "   8. Decision Tree (Balanced Data)\n",
            "   9. XGBClassifier (Class Weighted)\n",
            "   10. XGBClassifier (Balanced Data)\n",
            "   11. Simple LSTM (Class Weighted)\n",
            "   12. Simple LSTM (Balanced Data)\n",
            "\n",
            "🔧 Starting model training with MLflow logging...\n",
            "🤖 Training multiple machine learning models...\n",
            "\n",
            "🔧 Training Random Forest (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.917\n",
            "   Val F1: 0.400\n",
            "\n",
            "🔧 Training Random Forest (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.588\n",
            "\n",
            "🔧 Training Logistic Regression (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 0.924\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.588\n",
            "\n",
            "🔧 Training Logistic Regression (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.917\n",
            "   Val F1: 0.667\n",
            "\n",
            "🔧 Training Gradient Boosting (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.931\n",
            "   Val F1: 0.615\n",
            "\n",
            "🔧 Training Gradient Boosting (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.632\n",
            "\n",
            "🔧 Training Decision Tree (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 0.983\n",
            "   Val Accuracy: 0.792\n",
            "   Val F1: 0.348\n",
            "   ⚠️ Potential overfitting detected (gap: 0.191)\n",
            "\n",
            "🔧 Training Decision Tree (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.903\n",
            "   Val F1: 0.632\n",
            "\n",
            "🔧 Training XGBClassifier (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 0.993\n",
            "   Val Accuracy: 0.931\n",
            "   Val F1: 0.706\n",
            "\n",
            "🔧 Training XGBClassifier (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 1.000\n",
            "   Val Accuracy: 0.917\n",
            "   Val F1: 0.667\n",
            "\n",
            "🔧 Training Simple LSTM (Class Weighted)...\n",
            "   Training on 288 samples with 2 classes\n",
            "   📊 LSTM input shape: (288, 19)\n",
            "   📊 Target shape: (288,)\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 0.844\n",
            "   Val Accuracy: 0.875\n",
            "   Val F1: 0.571\n",
            "\n",
            "🔧 Training Simple LSTM (Balanced Data)...\n",
            "   Training on 70 samples with 2 classes\n",
            "   📊 LSTM input shape: (70, 19)\n",
            "   📊 Target shape: (70,)\n",
            "   ✅ Training successful!\n",
            "   Train Accuracy: 0.829\n",
            "   Val Accuracy: 0.847\n",
            "   Val F1: 0.421\n",
            "\n",
            "📊 Training Results Summary:\n",
            "Models attempted: 12\n",
            "Successful trainings: 12\n",
            "Failed trainings: 0\n",
            "\n",
            "\n",
            "📊 Training Results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Status</th>\n",
              "      <th>Train_Accuracy</th>\n",
              "      <th>Val_Accuracy</th>\n",
              "      <th>Val_Precision</th>\n",
              "      <th>Val_Recall</th>\n",
              "      <th>Val_F1</th>\n",
              "      <th>Training_Samples</th>\n",
              "      <th>Classes_in_Training</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logistic Regression (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.923611</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Logistic Regression (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gradient Boosting (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Decision Tree (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.982639</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.347826</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Decision Tree (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>XGBClassifier (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.993056</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>XGBClassifier (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Simple LSTM (Class Weighted)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Simple LSTM (Balanced Data)</td>\n",
              "      <td>Success</td>\n",
              "      <td>0.828571</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Model   Status  Train_Accuracy  \\\n",
              "0         Random Forest (Class Weighted)  Success        1.000000   \n",
              "1          Random Forest (Balanced Data)  Success        1.000000   \n",
              "2   Logistic Regression (Class Weighted)  Success        0.923611   \n",
              "3    Logistic Regression (Balanced Data)  Success        1.000000   \n",
              "4     Gradient Boosting (Class Weighted)  Success        1.000000   \n",
              "5      Gradient Boosting (Balanced Data)  Success        1.000000   \n",
              "6         Decision Tree (Class Weighted)  Success        0.982639   \n",
              "7          Decision Tree (Balanced Data)  Success        1.000000   \n",
              "8         XGBClassifier (Class Weighted)  Success        0.993056   \n",
              "9          XGBClassifier (Balanced Data)  Success        1.000000   \n",
              "10          Simple LSTM (Class Weighted)  Success        0.843750   \n",
              "11           Simple LSTM (Balanced Data)  Success        0.828571   \n",
              "\n",
              "    Val_Accuracy  Val_Precision  Val_Recall    Val_F1  Training_Samples  \\\n",
              "0       0.916667       0.500000    0.333333  0.400000               288   \n",
              "1       0.902778       0.454545    0.833333  0.588235                70   \n",
              "2       0.902778       0.454545    0.833333  0.588235               288   \n",
              "3       0.916667       0.500000    1.000000  0.666667                70   \n",
              "4       0.930556       0.571429    0.666667  0.615385               288   \n",
              "5       0.902778       0.461538    1.000000  0.631579                70   \n",
              "6       0.791667       0.235294    0.666667  0.347826               288   \n",
              "7       0.902778       0.461538    1.000000  0.631579                70   \n",
              "8       0.930556       0.545455    1.000000  0.705882               288   \n",
              "9       0.916667       0.500000    1.000000  0.666667                70   \n",
              "10      0.875000       0.400000    1.000000  0.571429               288   \n",
              "11      0.847222       0.307692    0.666667  0.421053                70   \n",
              "\n",
              "    Classes_in_Training  \n",
              "0                     2  \n",
              "1                     2  \n",
              "2                     2  \n",
              "3                     2  \n",
              "4                     2  \n",
              "5                     2  \n",
              "6                     2  \n",
              "7                     2  \n",
              "8                     2  \n",
              "9                     2  \n",
              "10                    2  \n",
              "11                    2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/25 02:00:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🏆 Best performing model: XGBClassifier (Class Weighted)\n",
            "   Validation F1: 0.7059\n",
            "   Validation Accuracy: 0.9306\n",
            "💾 Saving best model 'XGBClassifier (Class Weighted)' to MLflow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/25 02:01:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        }
      ],
      "source": [
        "# Configure and train multiple models\n",
        "print(\"🤖 Configuring models for training...\")\n",
        "\n",
        "try:\n",
        "    # Get model configurations\n",
        "    models_config = get_model_configurations(data_results)\n",
        "    print(f\"✅ Created {len(models_config)} model configurations\")\n",
        "\n",
        "    # List configured models\n",
        "    print(\"\\n📋 Models to train:\")\n",
        "    for i, model_name in enumerate(models_config.keys(), 1):\n",
        "        print(f\"   {i}. {model_name}\")\n",
        "\n",
        "    trained_models = {}\n",
        "    print(\"\\n🔧 Starting model training with MLflow logging...\")\n",
        "\n",
        "    mlflow.set_experiment(\"churn_model_training\")\n",
        "    with mlflow.start_run(run_name=\"train_all_models\"):\n",
        "        mlflow.log_param(\"num_models\", len(models_config))\n",
        "\n",
        "        # Train and evaluate all models\n",
        "        trained_models, results_df = train_and_evaluate_models(models_config, X_val, y_val)\n",
        "\n",
        "        # Log performance metrics for each\n",
        "        for _, row in results_df.iterrows():\n",
        "            if row.Status == \"Success\":\n",
        "                # sanitize model name\n",
        "                safe_name = row.Model.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "                mlflow.log_metric(f\"{safe_name}_val_f1\", float(row.Val_F1))\n",
        "                mlflow.log_metric(f\"{safe_name}_val_accuracy\", float(row.Val_Accuracy))\n",
        "\n",
        "        print(\"\\n📊 Training Results:\")\n",
        "        display(results_df)\n",
        "\n",
        "        # Identify and log the best model\n",
        "        successful = results_df[results_df.Status == \"Success\"]\n",
        "        if not successful.empty:\n",
        "            best_idx = successful.Val_F1.idxmax()\n",
        "            best_model_name = successful.loc[best_idx, \"Model\"]\n",
        "            best_f1 = float(successful.loc[best_idx, \"Val_F1\"])\n",
        "            best_acc = float(successful.loc[best_idx, \"Val_Accuracy\"])\n",
        "            mlflow.log_metric(\"best_model_val_f1\", best_f1)\n",
        "            mlflow.log_metric(\"best_model_val_accuracy\", best_acc)\n",
        "            mlflow.log_param(\"best_model_name\", best_model_name)\n",
        "\n",
        "            print(f\"\\n🏆 Best performing model: {best_model_name}\")\n",
        "            print(f\"   Validation F1: {best_f1:.4f}\")\n",
        "            print(f\"   Validation Accuracy: {best_acc:.4f}\")\n",
        "            best_model = trained_models[best_model_name]\n",
        "        else:\n",
        "            print(\"❌ No models trained successfully!\")\n",
        "            best_model = None\n",
        "\n",
        "    print(f\"💾 Saving best model '{best_model_name}' to MLflow\")\n",
        "    if best_model is not None:\n",
        "        mlflow.sklearn.log_model(best_model, \"best_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Model training failed: {e}\")\n",
        "    print(\"💡 Check data preparation and model configurations\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-25 02:01:08,223] A new study created in memory with name: no-name-432992b8-5e75-4d2d-a198-a2099f82e7bf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Starting hyperparameter tuning on the best model only...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-25 02:01:08,553] Trial 0 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 235, 'learning_rate': 0.017351135471907687, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.9300320009190989, 'colsample_bytree': 0.9422352567087249, 'gamma': 4.151588748526681, 'reg_alpha': 1.9855125791462247, 'reg_lambda': 6.371234096524346}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:08,960] Trial 1 finished with value: 0.0 and parameters: {'n_estimators': 365, 'learning_rate': 0.0017391499475844616, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.8462058267859722, 'colsample_bytree': 0.5874533801279715, 'gamma': 4.349093182733871, 'reg_alpha': 4.006101385026467, 'reg_lambda': 4.00266275540874}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:09,418] Trial 2 finished with value: 0.0 and parameters: {'n_estimators': 458, 'learning_rate': 0.0010304211637789217, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.7616831495747722, 'colsample_bytree': 0.8237408408759435, 'gamma': 3.889907738550833, 'reg_alpha': 7.896433095148212, 'reg_lambda': 6.108378423838879}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:09,799] Trial 3 finished with value: 0.0 and parameters: {'n_estimators': 343, 'learning_rate': 0.011344659121714793, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.938010756571137, 'colsample_bytree': 0.6105144240458096, 'gamma': 4.6666881713558865, 'reg_alpha': 8.221508188669839, 'reg_lambda': 1.0049342380675874}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:10,074] Trial 4 finished with value: 0.36363636363636365 and parameters: {'n_estimators': 115, 'learning_rate': 0.27308043884023264, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.8487604245363249, 'colsample_bytree': 0.8328143088110204, 'gamma': 2.819030625722801, 'reg_alpha': 5.35142059701874, 'reg_lambda': 9.006268741919794}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:10,517] Trial 5 finished with value: 0.0 and parameters: {'n_estimators': 540, 'learning_rate': 0.2665601032106087, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7167151671437966, 'colsample_bytree': 0.5550713986058603, 'gamma': 2.284985780999664, 'reg_alpha': 7.0533484709646475, 'reg_lambda': 5.398914810340877}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:10,812] Trial 6 finished with value: 0.0 and parameters: {'n_estimators': 158, 'learning_rate': 0.001310844514121673, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.6358538692837121, 'colsample_bytree': 0.6108893855889483, 'gamma': 1.9028983966175173, 'reg_alpha': 7.633346790026523, 'reg_lambda': 8.95384131771923}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:11,127] Trial 7 finished with value: 0.0 and parameters: {'n_estimators': 146, 'learning_rate': 0.0018610469579320142, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.9603992431624038, 'colsample_bytree': 0.6584596268489046, 'gamma': 3.299872344625911, 'reg_alpha': 5.273472893247649, 'reg_lambda': 6.8936744475864264}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:11,500] Trial 8 finished with value: 0.0 and parameters: {'n_estimators': 354, 'learning_rate': 0.20602462149219353, 'max_depth': 10, 'min_child_weight': 4, 'subsample': 0.9081226631792216, 'colsample_bytree': 0.8152902604358006, 'gamma': 3.427210857097048, 'reg_alpha': 9.302570391023082, 'reg_lambda': 1.9752732290843622}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:11,936] Trial 9 finished with value: 0.0 and parameters: {'n_estimators': 456, 'learning_rate': 0.01719996606240023, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.6962375379763105, 'colsample_bytree': 0.5846381017095976, 'gamma': 3.1075228711108425, 'reg_alpha': 7.688624825890589, 'reg_lambda': 6.838135812669545}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:12,270] Trial 10 finished with value: 0.0 and parameters: {'n_estimators': 201, 'learning_rate': 0.04829977924590036, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.5331234951211176, 'colsample_bytree': 0.9975030623488442, 'gamma': 0.3786630821512311, 'reg_alpha': 0.25741501100588504, 'reg_lambda': 3.7039888244339827}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:12,557] Trial 11 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 85, 'learning_rate': 0.07210111392724072, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.8333390203038746, 'colsample_bytree': 0.9469132525112842, 'gamma': 1.4843922178031017, 'reg_alpha': 1.9379817117704197, 'reg_lambda': 9.540737838579632}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:12,916] Trial 12 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 263, 'learning_rate': 0.05862009047780353, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9994954852771448, 'colsample_bytree': 0.9948688904610059, 'gamma': 1.3476132002629468, 'reg_alpha': 1.3103372370153439, 'reg_lambda': 9.74961123082632}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:13,200] Trial 13 finished with value: 0.0 and parameters: {'n_estimators': 55, 'learning_rate': 0.04997766627396336, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.8271490004615648, 'colsample_bytree': 0.918753823530168, 'gamma': 1.0136267941772301, 'reg_alpha': 2.55738723126239, 'reg_lambda': 7.569522459763418}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:13,604] Trial 14 finished with value: 0.0 and parameters: {'n_estimators': 266, 'learning_rate': 0.0068934985867909965, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.7952166034613476, 'colsample_bytree': 0.9106016937781907, 'gamma': 1.7111687289366455, 'reg_alpha': 2.9610607079617015, 'reg_lambda': 8.142002436456458}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:13,889] Trial 15 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 83, 'learning_rate': 0.10431703495235023, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.8957092161772661, 'colsample_bytree': 0.7384305041117674, 'gamma': 0.34471654782462924, 'reg_alpha': 1.578554225798014, 'reg_lambda': 3.6387969223543717}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:14,257] Trial 16 finished with value: 0.0 and parameters: {'n_estimators': 214, 'learning_rate': 0.005721660190660987, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8840007402466934, 'colsample_bytree': 0.911014564912574, 'gamma': 2.3781305682590954, 'reg_alpha': 3.222299156409962, 'reg_lambda': 5.118057658159903}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:14,669] Trial 17 finished with value: 0.3333333333333333 and parameters: {'n_estimators': 258, 'learning_rate': 0.026714938699359245, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.9982833420715276, 'colsample_bytree': 0.7375040061906485, 'gamma': 4.059307608659683, 'reg_alpha': 0.11389497827488371, 'reg_lambda': 9.842111700635918}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:15,000] Trial 18 finished with value: 0.0 and parameters: {'n_estimators': 186, 'learning_rate': 0.1134211455686369, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.6333635366056785, 'colsample_bytree': 0.9480459048088355, 'gamma': 4.884403816932273, 'reg_alpha': 4.298027745868172, 'reg_lambda': 8.089992383603736}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:15,332] Trial 19 finished with value: 0.2222222222222222 and parameters: {'n_estimators': 111, 'learning_rate': 0.023230936163894107, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.7884038727597492, 'colsample_bytree': 0.8717337742594279, 'gamma': 0.9681063623564056, 'reg_alpha': 1.6706275399808075, 'reg_lambda': 2.746859532259829}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:15,871] Trial 20 finished with value: 0.0 and parameters: {'n_estimators': 599, 'learning_rate': 0.006056033590808104, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.5145383647096461, 'colsample_bytree': 0.7851034054349915, 'gamma': 3.6669876056493416, 'reg_alpha': 6.353940662115914, 'reg_lambda': 6.086801517880827}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:16,240] Trial 21 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 279, 'learning_rate': 0.045257445903274365, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9895683635560871, 'colsample_bytree': 0.9917763305003174, 'gamma': 1.5303423979951034, 'reg_alpha': 1.4588090231368318, 'reg_lambda': 9.648196012491486}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:16,641] Trial 22 finished with value: 0.4 and parameters: {'n_estimators': 294, 'learning_rate': 0.09092316748003397, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9232240450077355, 'colsample_bytree': 0.9626205115859302, 'gamma': 1.0687351304032953, 'reg_alpha': 0.9524969748349126, 'reg_lambda': 8.688585963942092}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:17,064] Trial 23 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 413, 'learning_rate': 0.05864268437946817, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.9620171678597467, 'colsample_bytree': 0.8805442208649192, 'gamma': 1.361506133423949, 'reg_alpha': 2.687136272236825, 'reg_lambda': 9.824406495278511}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:17,433] Trial 24 finished with value: 0.0 and parameters: {'n_estimators': 207, 'learning_rate': 0.02896709487225199, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.8638298424964498, 'colsample_bytree': 0.9577446695401394, 'gamma': 2.071470998598236, 'reg_alpha': 2.1974575495117845, 'reg_lambda': 7.403814837669244}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:17,809] Trial 25 finished with value: 0.0 and parameters: {'n_estimators': 239, 'learning_rate': 0.0128667023522622, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.9439851847847198, 'colsample_bytree': 0.8672814578493084, 'gamma': 2.645638422941123, 'reg_alpha': 4.172786554272959, 'reg_lambda': 8.724064190847685}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:18,188] Trial 26 finished with value: 0.5 and parameters: {'n_estimators': 303, 'learning_rate': 0.14328559184215825, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.8140166045394802, 'colsample_bytree': 0.5007055700713686, 'gamma': 0.05402931706223035, 'reg_alpha': 0.679104641346823, 'reg_lambda': 6.269409205066632}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:18,532] Trial 27 finished with value: 0.5 and parameters: {'n_estimators': 153, 'learning_rate': 0.07576521637174186, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.8826262906068945, 'colsample_bytree': 0.9363826145084565, 'gamma': 1.2521890840052896, 'reg_alpha': 3.514526391828456, 'reg_lambda': 7.940819060707199}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:18,810] Trial 28 finished with value: 0.6153846153846154 and parameters: {'n_estimators': 53, 'learning_rate': 0.03575444965576615, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.9827383849264882, 'colsample_bytree': 0.6832799991748864, 'gamma': 0.6286786878705095, 'reg_alpha': 2.0341793420816083, 'reg_lambda': 0.08915426807535898}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:19,244] Trial 29 finished with value: 0.0 and parameters: {'n_estimators': 393, 'learning_rate': 0.002721494079693761, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.8531255679939567, 'colsample_bytree': 0.9962594903446074, 'gamma': 1.8859916266840886, 'reg_alpha': 1.0732749591522328, 'reg_lambda': 4.568507000340742}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:19,546] Trial 30 finished with value: 0.0 and parameters: {'n_estimators': 112, 'learning_rate': 0.1757099380164126, 'max_depth': 7, 'min_child_weight': 9, 'subsample': 0.9236981019038621, 'colsample_bytree': 0.8907260568037948, 'gamma': 4.306712197963656, 'reg_alpha': 3.7575913500146445, 'reg_lambda': 9.277243922199762}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:19,852] Trial 31 finished with value: 0.5714285714285714 and parameters: {'n_estimators': 83, 'learning_rate': 0.07412523819768486, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.8915484241547045, 'colsample_bytree': 0.7321816744607719, 'gamma': 0.4951574626518612, 'reg_alpha': 1.5847271326657038, 'reg_lambda': 3.5737019070154488}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:20,176] Trial 32 finished with value: 0.5 and parameters: {'n_estimators': 165, 'learning_rate': 0.13002166305764884, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9626779339401987, 'colsample_bytree': 0.6824121644447111, 'gamma': 0.7175288400485571, 'reg_alpha': 2.2510697438022267, 'reg_lambda': 2.8816760347216}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:20,562] Trial 33 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 316, 'learning_rate': 0.08377295223515366, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.7706706492684025, 'colsample_bytree': 0.781163540112184, 'gamma': 0.05089639884342567, 'reg_alpha': 0.8302528934770608, 'reg_lambda': 4.2922936274276005}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:20,963] Trial 34 finished with value: 0.0 and parameters: {'n_estimators': 238, 'learning_rate': 0.00990995668645497, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.900830725698356, 'colsample_bytree': 0.83254652483748, 'gamma': 1.4743365807858213, 'reg_alpha': 4.685477269299728, 'reg_lambda': 2.034276786803429}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:21,267] Trial 35 finished with value: 0.0 and parameters: {'n_estimators': 81, 'learning_rate': 0.01662786455981531, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.7326396505499999, 'colsample_bytree': 0.9722487093657071, 'gamma': 2.8177240023209897, 'reg_alpha': 1.6189202836235452, 'reg_lambda': 5.883367282599855}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:21,598] Trial 36 finished with value: 0.4 and parameters: {'n_estimators': 128, 'learning_rate': 0.033680777375502964, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.8293431643550749, 'colsample_bytree': 0.7701443573386046, 'gamma': 0.3422367951741459, 'reg_alpha': 0.009938488531264555, 'reg_lambda': 4.904123607172427}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:21,896] Trial 37 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 88, 'learning_rate': 0.2998662235638511, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.9399285883154099, 'colsample_bytree': 0.8449074994972497, 'gamma': 1.994660538942973, 'reg_alpha': 3.2292083773489844, 'reg_lambda': 3.0393377461167033}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:22,302] Trial 38 finished with value: 0.5714285714285714 and parameters: {'n_estimators': 337, 'learning_rate': 0.21164533469408325, 'max_depth': 2, 'min_child_weight': 8, 'subsample': 0.8485691790175636, 'colsample_bytree': 0.7003030577020144, 'gamma': 0.8769873177437797, 'reg_alpha': 6.036187365810996, 'reg_lambda': 7.027635267152429}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:22,635] Trial 39 finished with value: 0.5714285714285714 and parameters: {'n_estimators': 175, 'learning_rate': 0.10694385134986802, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.8678084889052252, 'colsample_bytree': 0.6358035792780822, 'gamma': 2.2080886842602263, 'reg_alpha': 1.2155619730014133, 'reg_lambda': 9.207223427832705}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:23,046] Trial 40 finished with value: 0.0 and parameters: {'n_estimators': 370, 'learning_rate': 0.06301949051820017, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.9177813480551158, 'colsample_bytree': 0.9397763391415396, 'gamma': 4.491182900732947, 'reg_alpha': 9.37915550694225, 'reg_lambda': 8.353511069125751}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:23,442] Trial 41 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 279, 'learning_rate': 0.03978172452430812, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.977400319276345, 'colsample_bytree': 0.9820605887964723, 'gamma': 1.4966501959225331, 'reg_alpha': 1.7635789350148035, 'reg_lambda': 9.537620689670241}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:23,824] Trial 42 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 231, 'learning_rate': 0.02179279251491161, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9904958530001401, 'colsample_bytree': 0.9973293007243005, 'gamma': 1.6463989834597335, 'reg_alpha': 2.4978224802909876, 'reg_lambda': 9.834942524020923}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:24,196] Trial 43 finished with value: 0.4 and parameters: {'n_estimators': 256, 'learning_rate': 0.042495168190554135, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.9482522615639954, 'colsample_bytree': 0.9302321850101868, 'gamma': 1.1903217947781688, 'reg_alpha': 0.41516677742519403, 'reg_lambda': 8.756472855036689}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:24,666] Trial 44 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 501, 'learning_rate': 0.05833650960200369, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.9666849183008808, 'colsample_bytree': 0.9708040928412527, 'gamma': 1.767449995862707, 'reg_alpha': 1.399969032494149, 'reg_lambda': 9.121354050501266}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:25,008] Trial 45 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 128, 'learning_rate': 0.1698242423671825, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.999350032597777, 'colsample_bytree': 0.9116170996361422, 'gamma': 3.0815098363087343, 'reg_alpha': 0.6224356703980984, 'reg_lambda': 1.7140113918310664}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:25,369] Trial 46 finished with value: 0.0 and parameters: {'n_estimators': 279, 'learning_rate': 0.10103020728312138, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.9251482575243548, 'colsample_bytree': 0.809617875799104, 'gamma': 2.595800070785284, 'reg_alpha': 8.695199215513771, 'reg_lambda': 5.648169130480415}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:25,818] Trial 47 finished with value: 0.0 and parameters: {'n_estimators': 321, 'learning_rate': 0.003833571341470992, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.8981575674788062, 'colsample_bytree': 0.8989148108964653, 'gamma': 0.7853390840498582, 'reg_alpha': 2.8783680214419864, 'reg_lambda': 6.412183507735907}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:26,178] Trial 48 finished with value: 0.0 and parameters: {'n_estimators': 190, 'learning_rate': 0.00900665420150638, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.9437572929313188, 'colsample_bytree': 0.8535811771912508, 'gamma': 3.7421148233056476, 'reg_alpha': 1.9295308014850068, 'reg_lambda': 9.987994578484711}. Best is trial 0 with value: 0.6666666666666666.\n",
            "[I 2025-08-25 02:01:26,607] Trial 49 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 368, 'learning_rate': 0.04526573182420676, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.687291154964625, 'colsample_bytree': 0.9506141429083752, 'gamma': 0.23306414557495808, 'reg_alpha': 1.1711202004137728, 'reg_lambda': 7.541212690500425}. Best is trial 0 with value: 0.6666666666666666.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ✅ Tuned XGBClassifier (Class Weighted)\n",
            "   📊 Best F1: 0.6667\n",
            "   🔧 Params: {'n_estimators': 235, 'learning_rate': 0.017351135471907687, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.9300320009190989, 'colsample_bytree': 0.9422352567087249, 'gamma': 4.151588748526681, 'reg_alpha': 1.9855125791462247, 'reg_lambda': 6.371234096524346}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/25 02:01:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/25 02:01:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Saving tuned model locally...\n",
            "   ✅ Saved tuned model as: tuned_XGBClassifier_Class_Weighted_20250825_020132.joblib\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# End any active run\n",
        "if mlflow.active_run():\n",
        "    mlflow.end_run()\n",
        "\n",
        "print(\"🔧 Starting hyperparameter tuning on the best model only...\")\n",
        "\n",
        "if 'X_train' in locals() and 'y_train' in locals():\n",
        "    try:\n",
        "        if 'best_model_name' not in locals():\n",
        "            raise ValueError(\"best_model_name not defined\")\n",
        "\n",
        "        # Extract config\n",
        "        config = models_config[best_model_name]\n",
        "        model_instance = config[\"model\"]\n",
        "        X_train_data, y_train_data = config[\"data\"]\n",
        "\n",
        "        # Derive type\n",
        "        model_class = model_instance.__class__\n",
        "        if isinstance(model_instance, RandomForestClassifier):\n",
        "            model_type = \"random_forest\"\n",
        "        elif isinstance(model_instance, GradientBoostingClassifier):\n",
        "            model_type = \"gradient_boosting\"\n",
        "        elif isinstance(model_instance, XGBClassifier):\n",
        "            model_type = \"xgboost\"\n",
        "        elif isinstance(model_instance, LogisticRegression):\n",
        "            model_type = \"logistic_regression\"\n",
        "        elif isinstance(model_instance, DecisionTreeClassifier):\n",
        "            model_type = \"decision_tree\"\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model class: {model_class}\")\n",
        "\n",
        "        # Tune\n",
        "        best_model, best_score, best_params = optuna_hyperparameter_tuning(\n",
        "            model_class, model_type,\n",
        "            X_train_data, y_train_data,\n",
        "            X_val, y_val,\n",
        "            n_trials=50\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ Tuned {best_model_name}\")\n",
        "        print(f\"   📊 Best F1: {best_score:.4f}\")\n",
        "        print(f\"   🔧 Params: {best_params}\")\n",
        "\n",
        "        final_model = best_model\n",
        "        mlflow.log_metric(\"best_val_f1\", best_score)\n",
        "        for p, v in best_params.items():\n",
        "            mlflow.log_param(p, v)\n",
        "        mlflow.sklearn.log_model(final_model, \"tuned_model\")\n",
        "\n",
        "        # Save locally\n",
        "        print(\"\\n💾 Saving tuned model locally...\")\n",
        "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        safe = best_model_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "        fname = f\"tuned_{safe}_{ts}.joblib\"\n",
        "        joblib.dump(final_model, fname)\n",
        "        print(f\"   ✅ Saved tuned model as: {fname}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Tuning failed: {e}\")\n",
        "        final_model = best_model if 'best_model' in locals() else None\n",
        "else:\n",
        "    print(\"⚠️ Please run data processing and initial training first\")\n",
        "    final_model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Evaluation and Comprehensive Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Evaluating final model on test set...\n",
            "🎯 Final Optimized Model Evaluation Results:\n",
            "   Accuracy:  0.9101\n",
            "   Precision: 0.9002\n",
            "   Recall:    0.9101\n",
            "   F1-Score:  0.9042\n",
            "   ROC-AUC:   0.9390\n",
            "\n",
            "Classification Report for Final Optimized Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95        81\n",
            "           1       0.50      0.38      0.43         8\n",
            "\n",
            "    accuracy                           0.91        89\n",
            "   macro avg       0.72      0.67      0.69        89\n",
            "weighted avg       0.90      0.91      0.90        89\n",
            "\n",
            "\n",
            "✅ Model evaluation completed!\n",
            "\n",
            "💾 COMPREHENSIVE MODEL SAVING\n",
            "==================================================\n",
            "✅ Final model saved: models\\final_churn_model_20250825_020132.joblib\n",
            "✅ Pickle backup saved: models\\final_churn_model_20250825_020132.pkl\n",
            "✅ Complete model package: output_files\\model_package_20250825_020132.joblib\n",
            "✅ Feature columns saved: output_files\\feature_columns_20250825_020132.json\n",
            "✅ Comprehensive report: output_files\\model_report_20250825_020132.json\n",
            "✅ Loading script saved: output_files\\load_model_20250825_020132.py\n",
            "💾 Preparing model for production deployment...\n",
            "✅ Saved model+metadata to ./churn_predictor_v1_production.pkl\n",
            "✅ Model deployed using deployment module!\n",
            "\n",
            "📦 MODEL SAVING SUMMARY\n",
            "========================================\n",
            "🤖 Model Type: XGBClassifier\n",
            "📊 Test F1-Score: 0.9042\n",
            "📊 Test Accuracy: 0.9101\n",
            "📊 Test Precision: 0.9002\n",
            "📊 Test Recall: 0.9101\n",
            "🔢 Features: 19\n",
            "📅 Saved: 2025-08-25 02:01:33\n",
            "📂 Files created: 7 (model, backup, package, features, report, script, metadata)\n",
            "\n",
            "✅ MODEL SUCCESSFULLY SAVED AND READY FOR PRODUCTION!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the final model on the test set and save it comprehensively\n",
        "if final_model is not None and 'X_test' in locals():\n",
        "    print(\"🎯 Evaluating final model on test set...\")\n",
        "    \n",
        "    try:\n",
        "        from eval import evaluate_churn_model\n",
        "        \n",
        "        final_metrics = evaluate_churn_model(\n",
        "            model=final_model,\n",
        "            X_test=X_test,\n",
        "            y_test=y_test,\n",
        "            model_name=\"Final Optimized Model\"\n",
        "        )\n",
        "        \n",
        "        print(\"\\n✅ Model evaluation completed!\")\n",
        "        \n",
        "        # Prepare comprehensive model information\n",
        "        model_info = {\n",
        "            'model': final_model,\n",
        "            'metrics': final_metrics,\n",
        "            'feature_columns': feature_columns,\n",
        "            'training_samples': len(X_train),\n",
        "            'validation_samples': len(X_val),\n",
        "            'test_samples': len(X_test),\n",
        "            'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'churn_rate': user_features_df['churn'].mean() if 'user_features_df' in locals() else 0.0,\n",
        "            'model_type': type(final_model).__name__,\n",
        "            'data_shape': X_train.shape\n",
        "        }\n",
        "        \n",
        "        # Comprehensive Model Saving\n",
        "        print(\"\\n💾 COMPREHENSIVE MODEL SAVING\")\n",
        "        print(\"=\" * 50)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        \n",
        "        # 1. Save the final model (primary method)\n",
        "        model_filename = f\"models\\\\final_churn_model_{timestamp}.joblib\"\n",
        "        try:\n",
        "            joblib.dump(final_model, model_filename)\n",
        "            print(f\"✅ Final model saved: {model_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Joblib save failed: {e}\")\n",
        "        \n",
        "        # 2. Save using pickle (backup method)\n",
        "        pickle_filename = f\"models\\\\final_churn_model_{timestamp}.pkl\"\n",
        "        try:\n",
        "            with open(pickle_filename, 'wb') as f:\n",
        "                pickle.dump(final_model, f)\n",
        "            print(f\"✅ Pickle backup saved: {pickle_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Pickle save failed: {e}\")\n",
        "        \n",
        "        # 3. Save complete model information package\n",
        "        info_filename = f\"output_files\\\\model_package_{timestamp}.joblib\"\n",
        "        try:\n",
        "            joblib.dump(model_info, info_filename)\n",
        "            print(f\"✅ Complete model package: {info_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Model package save failed: {e}\")\n",
        "        \n",
        "        # 4. Save feature columns (critical for deployment)\n",
        "        feature_filename = f\"output_files\\\\feature_columns_{timestamp}.json\"\n",
        "        try:\n",
        "            with open(feature_filename, 'w') as f:\n",
        "                json.dump(feature_columns, f, indent=2)\n",
        "            print(f\"✅ Feature columns saved: {feature_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Feature columns save failed: {e}\")\n",
        "        \n",
        "        # 5. Save comprehensive metrics and metadata\n",
        "        metrics_filename = f\"output_files\\\\model_report_{timestamp}.json\"\n",
        "        try:\n",
        "            comprehensive_report = {\n",
        "                'model_info': {\n",
        "                    'type': type(final_model).__name__,\n",
        "                    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'version': '1.0'\n",
        "                },\n",
        "                'data_info': {\n",
        "                    'total_users': len(user_features_df) if 'user_features_df' in locals() else 0,\n",
        "                    'training_samples': len(X_train),\n",
        "                    'validation_samples': len(X_val),\n",
        "                    'test_samples': len(X_test),\n",
        "                    'features_count': len(feature_columns),\n",
        "                    'churn_rate': user_features_df['churn'].mean() if 'user_features_df' in locals() else 0.0\n",
        "                },\n",
        "                'performance': final_metrics,\n",
        "                'features': feature_columns\n",
        "            }\n",
        "            with open(metrics_filename, 'w') as f:\n",
        "                json.dump(comprehensive_report, f, indent=2)\n",
        "            print(f\"✅ Comprehensive report: {metrics_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Report save failed: {e}\")\n",
        "        \n",
        "        # 6. Save model loading script\n",
        "        script_filename = f\"output_files\\\\load_model_{timestamp}.py\"\n",
        "        try:\n",
        "            loading_script = f'''#!/usr/bin/env python3\n",
        "\"\"\"\\nModel Loading Script for Churn Prediction Model\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\"\"\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_churn_model():\n",
        "    \"\"\"Load the trained churn prediction model and its metadata.\"\"\"\n",
        "    \n",
        "    # Load the model\n",
        "    model = joblib.load('{model_filename}')\n",
        "    \n",
        "    # Load feature columns\n",
        "    with open('{feature_filename}', 'r') as f:\n",
        "        feature_columns = json.load(f)\n",
        "    \n",
        "    # Load model report\n",
        "    with open('{metrics_filename}', 'r') as f:\n",
        "        model_report = json.load(f)\n",
        "    \n",
        "    print(f\"✅ Loaded model: {{model_report['model_info']['type']}}\")\n",
        "    print(f\"📊 Test F1 Score: {{model_report['performance']['f1']:.4f}}\")\n",
        "    print(f\"📊 Test Accuracy: {{model_report['performance']['accuracy']:.4f}}\")\n",
        "    print(f\"🔢 Features: {{len(feature_columns)}}\")\n",
        "    \n",
        "    return model, feature_columns, model_report\n",
        "\n",
        "def predict_churn(model, feature_columns, user_data):\n",
        "    \"\"\"Make churn predictions on new user data.\"\"\"\n",
        "    \n",
        "    # Ensure user_data has the required columns\n",
        "    if isinstance(user_data, dict):\n",
        "        user_data = pd.DataFrame([user_data])\n",
        "    \n",
        "    # Reorder columns to match training data\n",
        "    user_data = user_data.reindex(columns=feature_columns, fill_value=0)\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(user_data)\n",
        "    probability = model.predict_proba(user_data)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    return prediction, probability\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    model, features, report = load_churn_model()\n",
        "    print(f\"\\\\n🎯 Model ready for predictions!\")\n",
        "    print(f\"Example: prediction, probability = predict_churn(model, features, user_data_dict)\")\n",
        "'''\n",
        "            with open(script_filename, 'w') as f:\n",
        "                f.write(loading_script)\n",
        "            print(f\"✅ Loading script saved: {script_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Loading script save failed: {e}\")\n",
        "        \n",
        "        # 7. Try deployment module if available\n",
        "        if DEPLOYMENT_AVAILABLE:\n",
        "            try:\n",
        "                deploy_model(\n",
        "                    model=final_model,\n",
        "                    model_name=\"churn_predictor_v1\",\n",
        "                    feature_columns=feature_columns,\n",
        "                    performance_metrics=final_metrics\n",
        "                )\n",
        "                print(\"✅ Model deployed using deployment module!\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Deployment module failed: {e}\")\n",
        "        \n",
        "        # Summary\n",
        "        print(\"\\n📦 MODEL SAVING SUMMARY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"🤖 Model Type: {type(final_model).__name__}\")\n",
        "        print(f\"📊 Test F1-Score: {final_metrics['f1']:.4f}\")\n",
        "        print(f\"📊 Test Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "        print(f\"📊 Test Precision: {final_metrics['precision']:.4f}\")\n",
        "        print(f\"📊 Test Recall: {final_metrics['recall']:.4f}\")\n",
        "        print(f\"🔢 Features: {len(feature_columns)}\")\n",
        "        print(f\"📅 Saved: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"📂 Files created: 7 (model, backup, package, features, report, script, metadata)\")\n",
        "        \n",
        "        print(\"\\n✅ MODEL SUCCESSFULLY SAVED AND READY FOR PRODUCTION!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Final evaluation/saving failed: {e}\")\n",
        "        \n",
        "        # Emergency save - save whatever we have\n",
        "        print(\"\\n🚨 EMERGENCY SAVE PROCEDURE\")\n",
        "        if 'final_model' in locals() and final_model is not None:\n",
        "            emergency_filename = f\"models\\\\EMERGENCY_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "            try:\n",
        "                with open(emergency_filename, 'wb') as f:\n",
        "                    pickle.dump(final_model, f)\n",
        "                print(f\"🚨 Emergency model saved: {emergency_filename}\")\n",
        "            except:\n",
        "                print(\"❌ Emergency save also failed\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ No final model available or test data missing\")\n",
        "    print(\"💡 Please run all previous cells to train and tune a model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EDA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
