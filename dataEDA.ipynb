{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\"\"\n",
        "Author: Ahmed Alghaith\n",
        "Date: August 2025\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis for Music Streaming Churn\n",
        "\n",
        "This notebook provides comprehensive exploratory data analysis for the music streaming churn prediction dataset.\n",
        "\n",
        "**Author:** Ahmed Alghaith  \n",
        "**Date:** August 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "from utils import *\n",
        "from MusicStreamingEventProcessor import MusicStreamingEventProcessor\n",
        "\n",
        "# Setup plotting style\n",
        "setup_plotting_style()\n",
        "\n",
        "print(\"üì• Loading customer churn event data...\")\n",
        "# Load your data here - replace 'customer_churn.json' with your actual file path\n",
        "try:\n",
        "    events_df = pd.read_json('customer_churn.json', lines=True)\n",
        "    print(\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"üìä Loaded {len(events_df):,} events\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Data file not found. Please update the file path in the cell above.\")\n",
        "    print(\"üí° Expected file: 'customer_churn.json'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic data info\n",
        "print(f\"üìä Dataset shape: {events_df.shape}\")\n",
        "print(f\"üë• Unique users: {events_df['userId'].nunique()}\")\n",
        "print(f\"üìã Columns: {list(events_df.columns)}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nüîç Sample Data:\")\n",
        "display(events_df.head())\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nüìà Basic Statistics:\")\n",
        "display(events_df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "missing_data = events_df.isnull().sum()\n",
        "missing_summary = missing_data[missing_data > 0]\n",
        "if len(missing_summary) > 0:\n",
        "    print(missing_summary)\n",
        "else:\n",
        "    print(\"‚úÖ No missing values found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## User Behavior Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze user activity patterns\n",
        "print(\"üéµ User Activity Analysis\")\n",
        "\n",
        "# Page visits distribution\n",
        "if 'page' in events_df.columns:\n",
        "    page_counts = events_df['page'].value_counts()\n",
        "    print(f\"\\nüìä Total unique pages: {len(page_counts)}\")\n",
        "    print(\"\\nTop 10 most visited pages:\")\n",
        "    print(page_counts.head(10))\n",
        "    \n",
        "    # Visualize page visits\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    page_counts.head(15).plot(kind='bar')\n",
        "    plt.title('Top 15 Most Visited Pages')\n",
        "    plt.xlabel('Page')\n",
        "    plt.ylabel('Number of Visits')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è 'page' column not found in the data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subscription Level Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze subscription levels\n",
        "if 'level' in events_df.columns:\n",
        "    level_distribution = events_df['level'].value_counts()\n",
        "    print(\"üìä Subscription Level Distribution:\")\n",
        "    print(level_distribution)\n",
        "    \n",
        "    # Visualize subscription levels\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    level_distribution.plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title('Distribution of Subscription Levels')\n",
        "    plt.ylabel('')\n",
        "    plt.show()\n",
        "    \n",
        "    # User-level subscription analysis\n",
        "    user_levels = events_df.groupby('userId')['level'].agg(['first', 'last', 'nunique'])\n",
        "    level_changers = (user_levels['nunique'] > 1).sum()\n",
        "    print(f\"\\nüë• Users who changed subscription levels: {level_changers}\")\n",
        "    print(f\"üìà Percentage of level changers: {level_changers/len(user_levels)*100:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è 'level' column not found in the data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering and User Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and run the processor\n",
        "print(\"üè≠ Processing events to create user features...\")\n",
        "processor = MusicStreamingEventProcessor()\n",
        "\n",
        "try:\n",
        "    user_features_df = processor.process_events_to_features(events_df)\n",
        "    \n",
        "    print(f\"\\nüë• User features shape: {user_features_df.shape}\")\n",
        "    print(f\"üìä Features created: {len(user_features_df.columns)}\")\n",
        "    \n",
        "    # Display column names\n",
        "    print(f\"\\nüî¢ Feature columns:\")\n",
        "    for i, col in enumerate(user_features_df.columns, 1):\n",
        "        print(f\"  {i:2d}. {col}\")\n",
        "    \n",
        "    # Display sample of user features\n",
        "    print(\"\\nüìã Sample User Features:\")\n",
        "    display(user_features_df.head())\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error processing events: {str(e)}\")\n",
        "    print(\"üí° Please check your data format and try again\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Churn Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze churn distribution\n",
        "if 'user_features_df' in locals() and 'churn' in user_features_df.columns:\n",
        "    churn_distribution = user_features_df['churn'].value_counts()\n",
        "    print(\"üìä Churn Distribution:\")\n",
        "    print(churn_distribution)\n",
        "    print(f\"üìà Churn Rate: {churn_distribution.get(1, 0) / len(user_features_df) * 100:.2f}%\")\n",
        "    \n",
        "    # Visualize churn distribution\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    churn_distribution.plot(kind='bar')\n",
        "    plt.title('Churn Distribution')\n",
        "    plt.xlabel('Churn (0=No, 1=Yes)')\n",
        "    plt.ylabel('Number of Users')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional churn insights\n",
        "    if len(churn_distribution) > 1:\n",
        "        print(f\"\\nüí° Churn Insights:\")\n",
        "        print(f\"   ‚Ä¢ Churned users: {churn_distribution.get(1, 0):,}\")\n",
        "        print(f\"   ‚Ä¢ Active users: {churn_distribution.get(0, 0):,}\")\n",
        "        print(f\"   ‚Ä¢ Class balance ratio: {churn_distribution.get(0, 0) / max(churn_distribution.get(1, 1), 1):.2f}:1\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Churn column not found. Run feature engineering first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature correlations\n",
        "if 'user_features_df' in locals():\n",
        "    numeric_features = user_features_df.select_dtypes(include=[np.number])\n",
        "    \n",
        "    if len(numeric_features.columns) > 1:\n",
        "        # Correlation matrix\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        correlation_matrix = numeric_features.corr()\n",
        "        \n",
        "        # Create heatmap\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Show only lower triangle\n",
        "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, \n",
        "                    square=True, linewidths=0.5, cbar_kws={'shrink': 0.8}, fmt='.2f')\n",
        "        plt.title('Feature Correlation Matrix')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Show high correlations with churn if available\n",
        "        if 'churn' in correlation_matrix.columns:\n",
        "            churn_correlations = correlation_matrix['churn'].abs().sort_values(ascending=False)\n",
        "            print(\"\\nüéØ Top features correlated with churn:\")\n",
        "            top_correlations = churn_correlations[churn_correlations.index != 'churn'].head(10)\n",
        "            for feature, corr in top_correlations.items():\n",
        "                print(f\"  {feature}: {corr:.4f}\")\n",
        "        \n",
        "        # Show highly correlated feature pairs (potential multicollinearity)\n",
        "        print(\"\\nüîó Highly correlated feature pairs (|r| > 0.8):\")\n",
        "        high_corr_pairs = []\n",
        "        for i in range(len(correlation_matrix.columns)):\n",
        "            for j in range(i+1, len(correlation_matrix.columns)):\n",
        "                corr_val = correlation_matrix.iloc[i, j]\n",
        "                if abs(corr_val) > 0.8:\n",
        "                    high_corr_pairs.append((correlation_matrix.columns[i], \n",
        "                                           correlation_matrix.columns[j], \n",
        "                                           corr_val))\n",
        "        \n",
        "        if high_corr_pairs:\n",
        "            for feat1, feat2, corr_val in high_corr_pairs:\n",
        "                print(f\"  {feat1} ‚Üî {feat2}: {corr_val:.4f}\")\n",
        "        else:\n",
        "            print(\"  ‚úÖ No highly correlated pairs found\")\n",
        "            \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Insufficient numeric features for correlation analysis\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è User features not available. Run feature engineering first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provide summary and recommendations\n",
        "print(\"üìã EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'user_features_df' in locals():\n",
        "    print(f\"‚úÖ Successfully processed {len(events_df):,} events\")\n",
        "    print(f\"‚úÖ Created features for {len(user_features_df):,} users\")\n",
        "    print(f\"‚úÖ Generated {len(user_features_df.columns)} features\")\n",
        "    \n",
        "    if 'churn' in user_features_df.columns:\n",
        "        churn_rate = user_features_df['churn'].mean()\n",
        "        print(f\"‚úÖ Detected churn rate: {churn_rate:.2%}\")\n",
        "        \n",
        "        if 0.05 <= churn_rate <= 0.50:\n",
        "            print(\"‚úÖ Churn rate is suitable for machine learning\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Churn rate may need adjustment for optimal model performance\")\n",
        "    \n",
        "    print(\"\\nüéØ Ready for model training!\")\n",
        "    print(\"üìù Next steps:\")\n",
        "    print(\"  1. Run Training.ipynb for model development\")\n",
        "    print(\"  2. Consider feature selection based on correlations\")\n",
        "    print(\"  3. Handle class imbalance if needed\")\n",
        "    print(\"  4. Evaluate multiple algorithms\")\n",
        "else:\n",
        "    print(\"‚ùå Data processing incomplete\")\n",
        "    print(\"üìù Please check:\")\n",
        "    print(\"  1. Data file path and format\")\n",
        "    print(\"  2. Required columns are present\")\n",
        "    print(\"  3. No critical errors in processing\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EDA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
